---
layout: post
title: "I sneezed and now I can't move" a study of back pain predictors using machine learning
---



# <center>"I sneezed and now I can't move."</center>
## <center>A study of lower back pain using machine learning</center> 


![](https://images.duckduckgo.com/iu/?u=http%3A%2F%2Fcentralwestrehab.com.au%2Fimages%2Fuploads%2FChronic%2520Pain%2FBackpainCartoon.png&f=1)

With lower back pain it's either you have it or you know someone who has it. Our increasingly sedentary lifestyle has led to some apalling figures when it comes to the subject of back pains.  
According to [thegoodbody.com](www.thegoodbody.com), 8 in 10 Americans will experience back pain in their lifetime. They affect men almost as much as they affect women, reduce the ability to work, to focus, and are just unbearable. No need to convince anyone that lower back pain is a b\*\*\*\*.  

In this short study we'll try to get some leads on identifying the strongest factors causing lower back pain and we'll create models and representation to classify back pain as well as possible. To do so, we'll follow this **clickable** plan.


* **[Introduction](#Introduction)**
    1. [About the Data](#About-the-Data)
    2. [Packages used](#Packages-used)
    3. [Goals](#Goals)

* ** [Exploratory Data Analysis](#Exploratory-Data-Analysis)**
    1. [Class Distribution](#Class-Distribution)
    2. [Correlations](#Correlations)
    3. [Shallow tree intuition](#Shallow-tree-intuition)
    4. [Univariate exploration of relevant factors](#Univariate-exploration-of-relevant-factors)
        1. [Degree spondylolisthesis](#Degree-spondylolisthesis)
        2. [Sacral slope, Pelvic incidence, and Pelvic tilt](#Sacral-slope,-Pelvic-incidence,-and-Pelvic-tilt)
        3. [Pelvic radius](#Pelvic-radius)
        4. [Lumbar lordosis angle](#Lumbar-lordosis-angle)
* **[Feature Engineering](#Feature-Engineering)**
    1. [PCA](#PCA)
    2. [Polynomial Features](#Polynomial-Features)
    3. [Boxing Features](#Boxing-Features)
    4. [Effect on a simple model](#Effect-on-a-simple-model)

* **[Modelling](#Modelling)**
    1. [Choosing interpretability](#Choosing-interpretability)
    2. [Decision Tree](#Decision-Tree)
    3. [Logistic Regression](#Logistic-Regression)
* **[Visualisations](#)**
    1. [PCA on the new data](#PCA-on-the-new-data)
    2. [TSNE](#TSNE)
    3. [Spectral Embedding](#Spectral-Embedding)
    4. [MDS](#MDS)

## Introduction
### About the Data
The data set that we are going to study compiles 309 observations of patients with and without lower back pains. It contains 12 different predictors with **pretty complicated** names but we'll try to explain the most relevant of these predictors as we progress through the study. It also contains 1 target feature called "Attribute" that will get the value "Abnormal" when lower back pain is present and "Normal" when everything is ? Normal, you guessed it.  
The description of the data set explain a few of the reasons why lower back pain can occur : 
* The large nerve roots in the low back that go to the legs may be irritated
* The smaller nerves that supply the low back may be irritated
* The large paired lower back muscles (erector spinae) may be strained
* The bones, ligaments or joints may be damaged
* An intervertebral disc may be degenerating  

We'll see in the data set that we get mostly morphological/positional values, nothing **directly** telling us that one of the foremetionned things is happening but positional indicators that can definitely put us on the way. This data set does **not** contain any demographic information ( to my great regret ) about the subject observed. It would've been interesting as some of the values we're going to study typically change between men, women, and ethnicities.

### Packages used
Might as well put everything in one line and be done with it, here's a list of what we will use here


```python
# The classics
import numpy as np
import pandas as pd

# Visualisation tools
import seaborn as sns

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import axes3d
from matplotlib import style

import plotly
from plotly.offline import iplot, init_notebook_mode
from plotly.graph_objs import Scatter3d, Layout, Figure

import graphviz 

# Machine learning unavoidables
from sklearn.decomposition import KernelPCA, PCA
from sklearn.preprocessing import normalize,MinMaxScaler,PolynomialFeatures
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.feature_selection import SelectKBest, chi2

```

### Goals
Through this notebook, I want to be able to identify the key factors causing lower back pain as well and I want to be able to create understandable classifiers to further human understanding of the problem while keeping a good quality of classification.  
I will try to explain my thought process and my decision as thouroughly as possible so that everyone can learn something from this notebook, would it be in the subject of data science or just about back pain.

## Exploratory Data Analysis


```python
# Defining the name of each column as it was given in the dataset
col_list = ['Pelvic_incidence',
               'Pelvic_tilt',
               'Lumbar_lordosis_angle',
               'Sacral_slope',
               'Pelvic_radius',
               'Degree_spondylolisthesis',
               'Pelvic_slope',
               'Direct_tilt',
               'Thoracic_slope',
               'Cervical_tilt',
               'Sacrum_angle',
               'Scoliosis_slope',
               'Attribute',
               'To_drop']

# Loading the data
data = pd.read_csv("../input/Dataset_spine.csv", names=col_list, header=1)

# The last column contained meta-data about the other columns and is irrelevant in our study
data.drop('To_drop', axis=1, inplace=True)


data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pelvic_incidence</th>
      <th>Pelvic_tilt</th>
      <th>Lumbar_lordosis_angle</th>
      <th>Sacral_slope</th>
      <th>Pelvic_radius</th>
      <th>Degree_spondylolisthesis</th>
      <th>Pelvic_slope</th>
      <th>Direct_tilt</th>
      <th>Thoracic_slope</th>
      <th>Cervical_tilt</th>
      <th>Sacrum_angle</th>
      <th>Scoliosis_slope</th>
      <th>Attribute</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39.056951</td>
      <td>10.060991</td>
      <td>25.015378</td>
      <td>28.995960</td>
      <td>114.405425</td>
      <td>4.564259</td>
      <td>0.415186</td>
      <td>12.8874</td>
      <td>17.5323</td>
      <td>16.78486</td>
      <td>-25.530607</td>
      <td>16.1102</td>
      <td>Abnormal</td>
    </tr>
    <tr>
      <th>1</th>
      <td>68.832021</td>
      <td>22.218482</td>
      <td>50.092194</td>
      <td>46.613539</td>
      <td>105.985135</td>
      <td>-3.530317</td>
      <td>0.474889</td>
      <td>26.8343</td>
      <td>17.4861</td>
      <td>16.65897</td>
      <td>-29.031888</td>
      <td>19.2221</td>
      <td>Abnormal</td>
    </tr>
    <tr>
      <th>2</th>
      <td>69.297008</td>
      <td>24.652878</td>
      <td>44.311238</td>
      <td>44.644130</td>
      <td>101.868495</td>
      <td>11.211523</td>
      <td>0.369345</td>
      <td>23.5603</td>
      <td>12.7074</td>
      <td>11.42447</td>
      <td>-30.470246</td>
      <td>18.8329</td>
      <td>Abnormal</td>
    </tr>
    <tr>
      <th>3</th>
      <td>49.712859</td>
      <td>9.652075</td>
      <td>28.317406</td>
      <td>40.060784</td>
      <td>108.168725</td>
      <td>7.918501</td>
      <td>0.543360</td>
      <td>35.4940</td>
      <td>15.9546</td>
      <td>8.87237</td>
      <td>-16.378376</td>
      <td>24.9171</td>
      <td>Abnormal</td>
    </tr>
    <tr>
      <th>4</th>
      <td>40.250200</td>
      <td>13.921907</td>
      <td>25.124950</td>
      <td>26.328293</td>
      <td>130.327871</td>
      <td>2.230652</td>
      <td>0.789993</td>
      <td>29.3230</td>
      <td>12.0036</td>
      <td>10.40462</td>
      <td>-1.512209</td>
      <td>9.6548</td>
      <td>Abnormal</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Checking for the integrity of the data is good practice
data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 309 entries, 0 to 308
    Data columns (total 13 columns):
    Pelvic_incidence            309 non-null float64
    Pelvic_tilt                 309 non-null float64
    Lumbar_lordosis_angle       309 non-null float64
    Sacral_slope                309 non-null float64
    Pelvic_radius               309 non-null float64
    Degree_spondylolisthesis    309 non-null float64
    Pelvic_slope                309 non-null float64
    Direct_tilt                 309 non-null float64
    Thoracic_slope              309 non-null float64
    Cervical_tilt               309 non-null float64
    Sacrum_angle                309 non-null float64
    Scoliosis_slope             309 non-null float64
    Attribute                   309 non-null object
    dtypes: float64(12), object(1)
    memory usage: 31.5+ KB
    

### Class Distribution


```python
sns.set_style("white")
g=sns.factorplot(x='Attribute', hue='Attribute', data= data, kind='count',size=5,aspect=.8)
```


![png](output_9_0.png)


From a simple look, we could tell that the proportions are 2/3 Abnormal and 1/3 Normal which is actually quite unusual for medical data. Usually the Abnormal cases are extremely rare and this creates very skewed data sets. We don't have any information on how the dataset was gathered and created so unfortunately we can't really know why these numbers seem so unusual.  

We'll get to the true count of each class later.


```python
# Replacing our attribute with binary values : 
data['Attribute'] = data['Attribute'].map({'Abnormal': 1, 'Normal': 0})
```

### Correlations
I find it always interesting to plot a correlation map when possible. They are very simple and easily readable and can provide a tremendous amount of information in just a look.


```python
sns.set(style="white")
d = data
corr = d.corr()
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(11, 9))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
g=sns.heatmap(corr, mask=mask, cmap=cmap, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
```


![png](output_13_0.png)


The first thing we want to look at is how our features are correlated with our target. As they are represented on the last line of this heatmap they are quite easy to see.
This way we identify that : 
* Pelvic_incidence
* Pelvic_tilt
* Lumbar_lordosis_angle
* Sacral_slope
* Pelvic_radius
* Degree_spondylolisthesis

Are all strongly correlated with our target values, with Degree_spondylolisthesis being the strongest one and Pelvic_radius being a negative correlation.

Secondly, we might want to look at other correlations, maybe some features are completely correlated and redundant, thus, not that usefull in a classification problem...  
The strongest correlations that I see between predictors is the one between Sacral_slope, Pelvic_Tilt, and Pelvic_incidence, we will definitely look into that later.
            
Hold down to you hats because it gets **fancier**, the package `seaborn` comes with a great tool to visualize more accurately the correlation inside of a dataset.

### More correlations !


```python
sns.pairplot(data, hue="Attribute")
plt.show()
```


![png](output_15_0.png)


It may seem a bit much at first sight, but taking each cell independently this graph is actually very understandable and holds even more information than the correlation map we plotted before.  
As I was about to plot this I was somewhat hoping for a **holy grail** of classification, a natural feature that would separate almost perfectly the two classes of attributes, but alas it wasn't here... But it doesn't mean that this graph was for nothing.  
It shows us _more or less_ some kind of linear relations between the first 6 columns of the data set.  
It also shows us the distribution of these 6 first columns and the separation that appears there between Normal and Abnormal albeit not perfect but always useful.

## Shallow tree intuition
Granted, we got to look at pretty much everything above and we got a pretty good hunch of which parameters were going to be important and which were not.   
But **looking** is not **knowing**. Thus, to get a deeper, more explainable intuition, we will create a graph for a shalllow decision tree. Because it is shallow the computation time and memory needed will be very small ( even though for a 309\*12 matrix this isn't our strongest concern), and it will only retain the most relevant factors.  
On a side note, I think this is **great** practice to do this to familiarise yourself with a dataset in a simple way.


```python
# Creating the arrays
X = data.iloc[:,:-1]
y = data.iloc[:,-1]

# Creating the shallow decision tree
clf = DecisionTreeClassifier(max_depth=3)

# Fitting the decision tree to the data set
clf = clf.fit(X, y)

# Plotting the results
dot = tree.export_graphviz(clf,out_file=None,
                         feature_names=col_list[:-2],  
                         class_names=['Normal','Abnormal'],  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot) 
graph.render("back problems")
graph
```




![svg](output_17_0.svg)



A true **beauty** that's for sure, but what do we get out of it ?
* Degree_spondylolisthesis is definitely a **key** factor in the understanding of lower back pain
* Sacral_slope is our second best split and will provide great information as well
* Cervical_tilt and Pelvic_radius seems interesting as well

I started this study with little domain knowledge, but I believe that cervical tilt refers to the upper vertabraes of the spine. We are talking about lower back pain but it is also possible that the cervical tilt would be a result of other displacements lower in the spine.

## Univariate exploration of relevant factors
### Degree spondylolisthesis
Spondylolisthesis is derived from the Greek words “spondy”, meaning vertebra, and “listhesis”, meaning movement. Spondylolisthesis is an abnormal condition in which there is instability in the spinal column, as one vertebral body is shifting forward over the next vertebrae.   
![tt](https://www.spine.org/portals/0/img/KnowYourBack/Conditions/LumbarSpondy1.png)Pardon my French, more simply, the degree of spondylolisthesis measures ( more or less ) how much a vertebra has slipped from its original position.  

Let's see how it is distributed among the classes.


```python
# A simple reusable function to plot the distribution of one feature with different colours for each class
def hist_graph(column):
    a4_dims = (11.7, 8.27)
    fig, ax = plt.subplots(figsize=a4_dims)
    sns.distplot(data[data['Attribute']==1][column], color='r')
    sns.distplot(data[data['Attribute']==0][column],ax=ax, color='b') 

hist_graph('Degree_spondylolisthesis')
```


![png](output_20_0.png)



```python
# A simple and reusable function to show the numbers in a dataframe
def compare_df(column):
    norm = data[data['Attribute']==0][[column]].describe()
    abnorm = data[data['Attribute']==1][[column]].describe()

    df = pd.DataFrame(data = norm)
    df['Normal'] = df[column]
    df.drop(column, axis=1, inplace=True)
    df['Abnormal']= abnorm
    return df

compare_df('Degree_spondylolisthesis')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Normal</th>
      <th>Abnormal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>209.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.186572</td>
      <td>37.959677</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.307483</td>
      <td>40.708721</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-11.058179</td>
      <td>-10.675871</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-1.511360</td>
      <td>7.918501</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.152710</td>
      <td>32.108537</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>4.968807</td>
      <td>55.506889</td>
    </tr>
    <tr>
      <th>max</th>
      <td>31.172767</td>
      <td>418.543082</td>
    </tr>
  </tbody>
</table>
</div>



For the Normal class : 

* The population is centered around 1.15.
* Small standard deviation showing a **very** centered classs (just see the histogram).
* The minimum value for the Normal class is lower than the minimum value for the Abnormal class showing that the degree spondylolisthesis doesn't have the same influence on back pains for a positive or for a negative displacement.
* The maximum value seems a bit high but since 75% of the Normal population is under ~5, these cases could be either outliers, or show that other factors can maybe compensate for the degree spondylolisthesis.

For the Abnormal class : 

* Both the mean and median are above the Normal's class maximum at respectively ~38 and ~32.
* High standard deviation showing a wide spread of the values and of the resulting issues.
* Still goes down in the negative values, then again showing little influence of this parameter once below zero.
* We'll take a closer look at that guy with the max value of 418...

### Sacral slope, Pelvic incidence, and Pelvic tilt

The sacral slope is the angle between a horizontal plane  and the upper surface of the sacrum. The sacrum is the last bone in our spine where the 5 vertebraes have fused together to create one bone. At the end of the sacrum, we can find the coccyx, remnant bone of what used to be a tail for human beings.
The sacral slope is closely related to the pelvic incidence and to the pelvic tilt by definition : 
$$ \text{Pelvic Incidence} = \text{Pelvic Tilt} + \text{Sacral Slope} $$
![sacral slope](https://musculoskeletalkey.com/wp-content/uploads/2016/07/C19-FF1-4.gif)

We don't have any information on the ethnicity of the patients in this data set, but it is good to notice that different values are found among different ethnicities for the sacral slope, this will help explain for variations of values that would seem abnormal otherwise. This is not only true for ethnicities but also for genders... A shame that we don't have any demographic data really.

### Sacral slope distribution


```python
hist_graph('Sacral_slope')
```


![png](output_24_0.png)



```python
compare_df('Sacral_slope')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Normal</th>
      <th>Abnormal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>209.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>38.863830</td>
      <td>44.922628</td>
    </tr>
    <tr>
      <th>std</th>
      <td>9.624004</td>
      <td>14.547159</td>
    </tr>
    <tr>
      <th>min</th>
      <td>17.386972</td>
      <td>13.366931</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>32.340487</td>
      <td>34.380345</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>37.059694</td>
      <td>44.644130</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>44.608788</td>
      <td>55.154267</td>
    </tr>
    <tr>
      <th>max</th>
      <td>67.195460</td>
      <td>121.429566</td>
    </tr>
  </tbody>
</table>
</div>



Not much stands out from the observation of the sacral slope alone. The Abnormal class usually has a slightly higher sacral slope than the normal class but it stays within one standard deviation of the Normal class.  
One thing we can notice is that then again, the Abnormal class is more widely spread than the Normal class and that we have among the Abnormal class one ( or several ) outliers around 120.

### Pelvic incidence distribution


```python
hist_graph('Pelvic_incidence')
```


![png](output_27_0.png)



```python
compare_df('Pelvic_incidence')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Normal</th>
      <th>Abnormal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>209.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>51.685244</td>
      <td>64.700527</td>
    </tr>
    <tr>
      <th>std</th>
      <td>12.368161</td>
      <td>17.704157</td>
    </tr>
    <tr>
      <th>min</th>
      <td>30.741938</td>
      <td>26.147921</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>42.817849</td>
      <td>50.066786</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>50.123115</td>
      <td>65.536003</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>61.470097</td>
      <td>77.655119</td>
    </tr>
    <tr>
      <th>max</th>
      <td>89.834676</td>
      <td>129.834041</td>
    </tr>
  </tbody>
</table>
</div>



Same thing as before, Normal class is more centered and usually lower than the Abnormal class which is again more widely spread.  
As we saw in the pairplot drawn earlier, there is almost never in this data set one single factor that will allow us to determine the normality or abnormality of a patient, but combinations of these parameters through feature engineering may help, even a human to tell at first glance the status of a patient.

### Pelvic tilt distribution


```python
hist_graph('Pelvic_tilt')
```


![png](output_30_0.png)



```python
compare_df('Pelvic_tilt')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Normal</th>
      <th>Abnormal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>209.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>12.821414</td>
      <td>19.777899</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.778503</td>
      <td>10.539372</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-5.845994</td>
      <td>-6.554948</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>8.799951</td>
      <td>13.040974</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>13.482435</td>
      <td>18.774071</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>16.785953</td>
      <td>24.822631</td>
    </tr>
    <tr>
      <th>max</th>
      <td>29.894119</td>
      <td>49.431864</td>
    </tr>
  </tbody>
</table>
</div>



A slight shift to the right for the Abnormal class that is again more widespread than the Normal one.
* While the Normal class seems to follow a Gaussian Distribution, the Abnormal class seems to follow more of a Poisson distribution with its longer tail on the right.
* Every case with a Pelvic tilt about 29.89 will be classified as Abnormal.
* Negative values show little impact by themselves.

### Pelvic radius
The pelvic radius is the angle formed between a vertical plane and the hip-axis/posterior superior corner of S1 ( first element of the sacrum ) line.  

![](https://synapse.koreamed.org/ArticleImage/0043JKOA/jkoa-51-9-g003-l.jpg)

<center>Pelvic radius is noted PR-S1 in this image</center>




Let's take a look at the distribution of the pelvic radius amon the different classes.


```python
hist_graph('Pelvic_radius')
```


![png](output_35_0.png)



```python
compare_df('Pelvic_radius')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Normal</th>
      <th>Abnormal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>209.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>123.890834</td>
      <td>115.156204</td>
    </tr>
    <tr>
      <th>std</th>
      <td>9.014246</td>
      <td>14.078340</td>
    </tr>
    <tr>
      <th>min</th>
      <td>100.501192</td>
      <td>70.082575</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>118.182659</td>
      <td>107.690466</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>123.874328</td>
      <td>115.723530</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>129.040401</td>
      <td>123.159251</td>
    </tr>
    <tr>
      <th>max</th>
      <td>147.894637</td>
      <td>163.071041</td>
    </tr>
  </tbody>
</table>
</div>



This is the first time that we are seeing the Abnormal class shifted to the left compared to the Normal class.
* Both have almost normal distributions, as per usual.
* The Abnormal class is still more widespread than the Normal class.
* The Normal class distribution is entirely contained into the Abnormal class distribution, giving us a lower and upper bound for the Normal class between ~100.5 and ~147.9.

### Lumbar lordosis angle
The lumbar lordosis angle is the angle formed by the intersection of two planes.
* The first plane is the one created following the upper surface of the L1 vertebra.
* The second plane is created by following the upper surface of the S1 vertebra.

![](http://sittingsafely.com/wp-content/uploads/2013/07/Lumbar-lordosis.png)


Let's see its distribution.


```python
hist_graph('Lumbar_lordosis_angle')
```


![png](output_40_0.png)



```python
compare_df('Lumbar_lordosis_angle')
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Normal</th>
      <th>Abnormal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>100.000000</td>
      <td>209.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>43.542605</td>
      <td>56.003438</td>
    </tr>
    <tr>
      <th>std</th>
      <td>12.361388</td>
      <td>19.684057</td>
    </tr>
    <tr>
      <th>min</th>
      <td>19.071075</td>
      <td>14.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>35.000000</td>
      <td>41.467855</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>42.638923</td>
      <td>56.300000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>51.602346</td>
      <td>68.118403</td>
    </tr>
    <tr>
      <th>max</th>
      <td>90.563461</td>
      <td>125.742385</td>
    </tr>
  </tbody>
</table>
</div>



We recognize here what we've been seeing previously in the most of the other parameters that we have studied.
* Higher values for the Abnormal class ( right-shift ).
* Higher spread for the Abnormal class.
* Normal class almost contained by the Abnormal class, giving us here just an upper bound where all patients with above  ~90 wil be classified as Abnormal.

Looking at the pairplot and at the correlation graph, we have drawn the most relevant distributions so far. The others parameters present very similar distribution for both Normal classes and Abnormal classes and their univariate study would not give us much insight.

Both our correlation plot and the shallow decision tree we used gave us an idea of what parameters were the most relevant to determine the Attribute of a patient. Let's try to create a new variable from the previous ones that will have even more weight for the predictions.

## Feature Engineering

Feature engineering is the process of creating new features from data to achieve better modelisation of the problem. It can be done before or after fitting the dataset to a simple model to see how each feature would impact on the quality of the model, but in the case of binary classification, we will see clearly in the histogram distribution if a new feature helps to separate the classes even more.
There are **lots** of ways to generate new features : 
* From domain knowledge
* From observation of the data
* From dimensionality reduction techniques such as PCA
* Just creating combinations of our current parameters

Now, I have no domain knowledge, and let's assume that we haven't really looked at the data yet for the fun of it, we'll try to create new features through PCA, LDA and polynomial combinations of our parameters

### PCA

In simple words, Principal Component Analysis or PCA tries to create linear combinations of the features in our data set to represent its variance as well as possible.  
For us, it can be a tool to try to create a feature that will separate the target as well as possible. That is, if PCA manages to represent the data set accurately.


```python
# Feature scaling
sc = MinMaxScaler()
X_std = sc.fit_transform(X)

# Creating the PCA
pca = PCA(n_components=3)

# Fitting the PCA to the data set
pca.fit(X_std)
```




    PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,
      svd_solver='auto', tol=0.0, whiten=False)



A good way to verify the quality of the PCA is by checking its explained_variance. This will show how much of the variance of the dataset the PCA manages to express for each of its component, thankfully our PCA object has a method `.explained_variance_ratio_`that does exactly this.


```python
pca.explained_variance_ratio_
```




    array([0.15836303, 0.1490722 , 0.13674482])



So this is **pretty terrible** to be honest.  
Even after adding the three components together we don't even get to 50% of explained variance for the data set...  
It doesn't mean that we should throw PCA away for this problem, other implementations of the PCA with the kernel trick ( kPCA in sklearn ) are also available, but I want to keep this notebook somewhat short so you'll have to look up this one by yourself.

### Polynomial Features
Creating new features from polynomial combinations and then filtering through them is also way of doing feature engineering. That being said, using `PolynomialFeatures`can be computationally expensive and slow. The reason behind this is that it is going to create all the combinations possible between all the selected features, including ( if you wish so ) power features from the previous ones ( understand squared, cubed, etc.) .

Here I specified that I wanted a maximum degree of 2 and I have 12 parameters.

I can expect:  
* ${12 \choose 2}=66$ New interacting features,  
* $12*1=12$ Squared features, and  
* $12$ of our old features.

That gives us a total of 90 features, let's check it.


```python
PF = PolynomialFeatures(degree=2, include_bias=False)
X_std_pf= PF.fit_transform(X_std)
new_feats = PF.get_feature_names()
X_std_pf.shape
```




    (309, 90)



Everything checks out with what I thought and that's great, **but** more features doesn't always mean a more accurate model. Some of them will introduce noise, some of them will make us overfit, some of them are just straight out bad.  

Now there are a lot of different methods to evaluate the quality of a given feature in regards to its target, and a lot of them happen actually **after** fitting an algorithm to the new data set and seeing an improvement or a loss in the quality of the algorithm ( here we will measure the quality of a classifier via the roc-auc value).

For the moment I will focus on trying to evaluate the quality of the new features **_a priori_** following the $\chi^2$ evaluation of these features and by keeping only the 10 best results.


```python
Kbest =  SelectKBest(chi2, k=10)
X_std_pf_chi10 = Kbest.fit_transform(X_std_pf, y)
selected = Kbest.get_support()
```


```python
features=[]
for feat, sel in zip(new_feats, selected) : 
    if sel == True :
        features.append(feat)

feat_col=[]
for i in features :
    split = i.split()
    if len(split)==1 :
        pow = split[0].split('^')
        if len(pow) == 1:
            nb =int(''.join([j for j in pow[0] if j.isdigit()]))
            col=data.columns[nb]
            feat_col.append(col)
        else :
            nb =int(''.join([j for j in pow[0] if j.isdigit()]))
            col=data.columns[nb]+'^'+pow[1]
            feat_col.append(col)
    else:
        clean =''.join([j for j in i if j.isdigit()])
        col=data.columns[int(clean[0])]+'*'+data.columns[int(clean[1])]
        feat_col.append(col)
```

The feature selected by the $\chi^2$ are as follow : 


```python
feat_col
```




    ['Degree_spondylolisthesis',
     'Pelvic_incidence^2',
     'Pelvic_incidence*Pelvic_tilt',
     'Pelvic_incidence*Lumbar_lordosis_angle',
     'Pelvic_incidence*Degree_spondylolisthesis',
     'Pelvic_tilt^2',
     'Pelvic_tilt*Lumbar_lordosis_angle',
     'Pelvic_tilt*Degree_spondylolisthesis',
     'Lumbar_lordosis_angle^2',
     'Lumbar_lordosis_angle*Degree_spondylolisthesis']



### Boxing Features
It is another technique that can be used to create new features. To do this we use the things we learned through our previous EDA to create new categorical features telling us if a value is above, below, or in between certain values.

For example, when we were looking at the distribution of the Sacral_slope amongst the classes, we realised that there was no Normal patient with a Sacral_slope value above 67.  
Thus, we can create a category 'ss>67' that would take 1 as a value if it is True and 0 if it is False.

Let's try to implement a few of these boxing features.


```python
box_deg = (data.Degree_spondylolisthesis > data[data['Attribute']==0].Degree_spondylolisthesis.max()).map({False: 0, True: 1})
box_ss  = ((data.Sacral_slope > data[data['Attribute']==0].Sacral_slope.max()) & (data.Sacral_slope > data[data['Attribute']==0].Sacral_slope.min())).map({False: 0, True: 1})
box_pi  = (data.Pelvic_incidence > data[data['Attribute']==0].Pelvic_incidence.max()).map({False: 0, True: 1})
box_pt  = ((data.Pelvic_tilt > data[data['Attribute']==0].Pelvic_tilt.max()) & (data.Pelvic_tilt > data[data['Attribute']==0].Pelvic_tilt.min())).map({False: 0, True: 1})
box_pr  = ((data.Pelvic_radius > data[data['Attribute']==0].Pelvic_radius.max()) & (data.Pelvic_radius > data[data['Attribute']==0].Pelvic_radius.min())).map({False: 0, True: 1})
box_lla = ((data.Lumbar_lordosis_angle > data[data['Attribute']==0].Lumbar_lordosis_angle.max()) & (data.Lumbar_lordosis_angle > data[data['Attribute']==0].Lumbar_lordosis_angle.min())).map({False: 0, True: 1})
X_box = np.array([box_deg,box_ss,box_pi,box_pt,box_pr,box_lla]).reshape(309,6)
```


```python
# Adding the boxing features to the other ones
X_std_box = np.hstack([X_std,X_box])
X_std_pf_chi10_box = np.hstack([X_std_pf_chi10,X_box])
```

### Effect on a simple model
Now that we have created these new features and these new sets of predictors, we would like to see their effect on the AUC of a simple classifier, since we used a Decision Tree earlier, we'll keep using it for simplicity reasons.  
Because the scores can have a high variance due to the randomization of some factors in the construction of the algorithm, the training data, and the testing data, we will cross-validate all the results we have to stabilize this variance and get insights from these results.


```python
# Creating a list through which we'll iterate the Decision Tree
X_list=[X_std, X_std_pf, X_std_pf_chi10,X_std_box, X_std_pf_chi10_box]
results = []

clf = DecisionTreeClassifier(random_state=42)

# Getting cross-validated scores for each of the new data sets
for X_set in X_list :
    clf.fit(X_set,y)
    y_pred = clf.predict(X_set)
    rez = cross_val_score(clf, X_set, y, scoring='roc_auc', cv=100 )
    results.append(rez.mean())
results
```




    [0.7075, 0.7308333333333333, 0.7775, 0.7341666666666665, 0.8025]



Let's take a look at the results above and try to understand them a little bit.
* The standard data set allows for a CV-AUC of 70.75%
* After adding 88 polynomial features it goes up ~2.25%
* After adding the boxing features it goes up ~2.5%

* Filtering the polynomial features with the standard data set make it go up by ~4.5%
* Adding the boxing features make the latter go up ~2.25%

First of all, even though this is cross validated and verified and all, after running it multiple times I was still getting a variance higher than I'd like on my results, so this is to take with a **grain of salt**. One thing that systematically came back though was the fact that either the filtered polynomial features or the filtered polynomial features + boxing would get the highest scores.

As we expected, the creation of polynomial features **increased** the base score, but not from much... How so ? It was certainly introducing a lot of **noise** and useless extra feature, this explanation is backed by the facte that once filtered through the $\chi2$ the score goes up **drastically**.

I am quite satisfied with the effect of the boxing features on the AUC score. Compared to the computational power required to first create all the polynomial features and then filter them, creating the boxing features was a **walk in the park** and their effect was definetely non negligeable on the outcome.

Now I want to keep this dataset simple without too many features, and even if the score is slightly better once the boxing features are added, I will use the filtered polynomial features without the boxing features for the modelling part of this notebook.

Here are the **winning** predictors with the targe in a DataFrame : 


```python
df_new = pd.DataFrame(X_std_pf_chi10, columns=feat_col)
df_new['Attribute'] = data['Attribute']
df_new.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Degree_spondylolisthesis</th>
      <th>Pelvic_incidence^2</th>
      <th>Pelvic_incidence*Pelvic_tilt</th>
      <th>Pelvic_incidence*Lumbar_lordosis_angle</th>
      <th>Pelvic_incidence*Degree_spondylolisthesis</th>
      <th>Pelvic_tilt^2</th>
      <th>Pelvic_tilt*Lumbar_lordosis_angle</th>
      <th>Pelvic_tilt*Degree_spondylolisthesis</th>
      <th>Lumbar_lordosis_angle^2</th>
      <th>Lumbar_lordosis_angle*Degree_spondylolisthesis</th>
      <th>Attribute</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.036365</td>
      <td>0.015501</td>
      <td>0.036950</td>
      <td>0.012273</td>
      <td>0.004527</td>
      <td>0.088080</td>
      <td>0.029256</td>
      <td>0.010793</td>
      <td>0.009718</td>
      <td>0.003585</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.017523</td>
      <td>0.169469</td>
      <td>0.211569</td>
      <td>0.132966</td>
      <td>0.007214</td>
      <td>0.264126</td>
      <td>0.165997</td>
      <td>0.009006</td>
      <td>0.104326</td>
      <td>0.005660</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.051838</td>
      <td>0.173182</td>
      <td>0.231968</td>
      <td>0.112885</td>
      <td>0.021572</td>
      <td>0.310710</td>
      <td>0.151204</td>
      <td>0.028895</td>
      <td>0.073582</td>
      <td>0.014062</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.044173</td>
      <td>0.051653</td>
      <td>0.065791</td>
      <td>0.029120</td>
      <td>0.010039</td>
      <td>0.083798</td>
      <td>0.037091</td>
      <td>0.012787</td>
      <td>0.016417</td>
      <td>0.005660</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.030933</td>
      <td>0.018499</td>
      <td>0.049745</td>
      <td>0.013541</td>
      <td>0.004207</td>
      <td>0.133769</td>
      <td>0.036413</td>
      <td>0.011314</td>
      <td>0.009912</td>
      <td>0.003080</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



## Modelling
### Choosing Interpretability

Some machine learning models can help people understand a problem more fully or in a different manner through the way they are constructed. These models are **interpretable** .  
Their decision process can be easily showed and understood to a human. Interpretable models can provide great insight, this is exactly what we did in the first part when building a shallow decision tree to then examine its decision process. Thanks to the shallow decision tree, we knew immediatly what parameters were the most relevant to our problem.  
One drawback is that most of the time these models are inherently more simple than others "black-box" models, and their results usually can't compare to more complex algorithms that lose in interpretability.  
In this part, we will use a **Decision Tree** and a **Logistic Regression** algorithm, tune them to maximize AUC,  and interpret their architecture or coefficients.

### Decision tree

The last time we used a Decision Tree with the newly engineered features we had an AUC ~80% . Now we want to get a better result and tune the algorithm to maximize the AUC. To do so, we will use the **GridSearchCV** from sklearn. It allows the construction of a parameter grid that will be applied and tested on the metrics we have specified ( in our case AUC ). Now, one has to be careful with GridSearch as it will train as many models as there are combinations of parameters in the parameter grid. That means it can take some time and can be computationally expensive. Since the data set is pretty small here, there is no problem with testing a few parameters at the same time. Once the GridSearch has finished testing everything, we look for the best parameters and ta-da, we tuned our algorithm.


```python
param_grid = {'max_depth': np.arange(1, 10),
             'min_samples_leaf' : np.arange(1, 10),
             'max_features' : ['auto','sqrt','log2',None],
             'random_state' : [37,]}

trees = GridSearchCV(DecisionTreeClassifier(), param_grid, scoring='roc_auc')
trees.fit(X_std_pf_chi10, y)

print("The best parameters are : ", trees.best_params_,' giving an AUC : ', trees.best_score_)
```

    The best parameters are :  {'max_depth': 3, 'max_features': 'auto', 'min_samples_leaf': 9, 'random_state': 37}  giving an AUC :  0.8193370900171982
    

Since our untuned Decision Tree, we got a **significant** increase for the AUC.  
As we've seen in an earlier part of this notebook, one of the **greatest** things with Decision Tree is that we can actually plot their architecture on a simple graph and interpret the results directly. This is exactly what we are going to do here :


```python
clf_tree = trees.best_estimator_

dot = tree.export_graphviz(clf_tree,out_file=None,
                         feature_names=df_new.columns[:-1],  
                         class_names=['Normal','Abnormal'], 
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot) 
graph.render("back problems2")
graph
```




![svg](output_65_0.svg)



From the top to the bottom we go from the most important parameters to the least important ones ( in regards to the Decision Tree algorithm ).

As such : 
* Degree_spondylolisthesis * Pelvic_incidence is the most influencial factor towards having back problems ( we've seen this result previously )
* Degree_spondylolisthesis is the second one

If you paid attention to all the parameters we were using, you probably realised that some parameters were missing from this tree.  
The Decision Tree as implemented in sklearn also comes with a method : `.feature_importances`. The name is pretty self-explanatory, let's dig into this.


```python
clf_tree.feature_importances_
```




    array([0.08210153, 0.04523111, 0.        , 0.02962147, 0.74705892,
           0.        , 0.        , 0.        , 0.09598696, 0.        ])



We see that five values are set to 0, they correspond to the columns : 
* Pelvic_incidence * Pelvic_tilt
* Pelvic_tilt^2
* Pelvic_tilt * Degree_spondylolisthesis
* Lumbar_lordosis_angle^2
* And Lumbar_lordosis_angle * Degree_spondylolisthesis

This isn't a mistake, it is simply due to the shallowness of the tree. Remember, the best parameters set a `'max_depth': 3`. With a deeper tree, these features would get a non-zero value for their importance, but probably still pretty close to zero. The fact that while doing GridSearch we got the best results for a shallower tree may show that the inclusion of the current zero features would have lead to overfitting the data set and thus reduce the AUC.

### Logistic Regression
We're also going to use GridSearchCV to optimize the AUC of the Logistic Regression, but the difference with the Decision Tree is that this time we will interpret the coefficients given to each feature by the algorithm instead of interpreting the architecture of it.


```python

param_grid = {'penalty': ['l1','l2'],
             'tol'     : [1e-5, 1e-4, 1e-3, 1e-2],
             'C'        : [1.0, 10.0, 25.0, 50.0, 100.0, 200.0, 500.0, 1000.0] ,
             'solver'    : ['liblinear',  'saga'],
             'random_state' : [37,],
             'max_iter' : [700,]}

logit = GridSearchCV(LogisticRegression(), param_grid, scoring='roc_auc',verbose=0)
logit.fit(X_std_pf_chi10, y)
```

    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    /opt/conda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning:
    
    The max_iter was reached which means the coef_ did not converge
    
    




    GridSearchCV(cv=None, error_score='raise',
           estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
              verbose=0, warm_start=False),
           fit_params=None, iid=True, n_jobs=1,
           param_grid={'penalty': ['l1', 'l2'], 'tol': [1e-05, 0.0001, 0.001, 0.01], 'C': [1.0, 10.0, 25.0, 50.0, 100.0, 200.0, 500.0, 1000.0], 'solver': ['liblinear', 'saga'], 'random_state': [37], 'max_iter': [700]},
           pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
           scoring='roc_auc', verbose=0)




```python
print("The best parameters are : ", logit.best_params_,' giving an AUC : ', logit.best_score_)
```

    The best parameters are :  {'C': 200.0, 'max_iter': 700, 'penalty': 'l1', 'random_state': 37, 'solver': 'liblinear', 'tol': 1e-05}  giving an AUC :  0.8734884485536286
    

For the tuned Logistic Regression, we get an AUC of ~87.3%, not bad.  
Let's see how we got to this result and check out the coefficient of each feature.


```python
clf = logit.best_estimator_
clf.coef_
```




    array([[ 46.92277766,  -1.03856755, -36.41247924,  30.54375694,
             76.55549212,  29.72691303, -18.55736995, -26.84365647,
             -9.71626654,  10.01455808]])



The array above represents the coefficients given to each of the parameters inside of the logistic regression. Their absolute value give an idea of the influence of each parameter while their sign tells us in which way they will influence the outcome.  

As such : 
* Degree_spondylolisthesis * Pelvic_incidence is the most influencial factor towards having back problems ( we've seen this result previously )
* Degree_spondylolisthesis  is the second one
* Pelvic_incidence * Pelvic_tilt_angle is the most influencial factor towards **not** having back problems closely followed by
* Pelvic_tilt * Degree_spondylolisthesis .

This information from the Decision Tree and from the Logistic Regression could help a practitioner decide the severity of one case or help define more specifically what the problem is and how to fix it for example.

It is interesting to notice that the two classifiers don't necessarily "agree" on the importance of each feature. This is due to the different way they compute and classify each case and hence fit to the data set. That being said, combining the interpretation of these different models can help refine even more our understanding of the problem on a human base.

## Visualisations
I have beeng playing with plotly for a while now and I really find that the interactivity of the plots adds so much to the reading and understanding experience of this very plot. It allows also to single out and identify outlier points in many cases for extended study later.  
We'll see some outlier points in the next visualisations in which a medical practitioner could be interested.  
If this patient is classified as normal, why is he so far from the normal cluster of points ?  
In one of my previous studies, after using an auto-encoder to perform dimmensionality reduction, I was able to redefine and create new classifications for a disease.  
As such, I believe that the study of different visualisations of the data sets through dimensionality reduction can also carry a great values

### PCA on the new data


```python
init_notebook_mode(connected=True)

X = df_new.iloc[:,:-1]
y = df_new.iloc[:,-1]

pca = PCA(n_components = 3)
X_PCA = pca.fit_transform(X)
    
xs = X_PCA[:,0]
ys = X_PCA[:,1]
zs = X_PCA[:,2]

# Recreating the df with the new coordinates
df = pd.DataFrame(dict(x=xs, y=ys, z=zs, Attribute=y)) 
l = []
names = ['Normal','Abnormal']

for i in [0,1]:    
    trace= Scatter3d(
        x= df[df['Attribute']==i]['x'],
        y= df[df['Attribute']==i]['y'],
        z= df[df['Attribute']==i]['z'],
        mode= 'markers',
        marker= dict(size= 5,
                    line= dict(width=1),
                    color= i,
                    colorscale='Jet',
                    opacity= 0.8
                   ),#name= y[i],
        name = names[i],
        text= df[df['Attribute']==i].index,# The hover text goes here...
        hoverinfo = 'text+name'
    )

    l.append(trace)

layout= Layout(
    title= '3D Representation of the patients characteristics using PCA',
    hovermode= 'closest',
    showlegend= True)

fig= Figure(data=l, layout=layout)
plotly.offline.iplot(fig)
print('The PCA explains the variance of the data by {:3f}%'.format(100*pca.explained_variance_ratio_.sum()))
```


<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>



<div id="931bb527-dc3d-4f07-8bcc-41f7b8399f99" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("931bb527-dc3d-4f07-8bcc-41f7b8399f99", [{"type": "scatter3d", "x": [-0.23109254354108402, -0.08739232663481304, -0.2506654045207038, -0.15224612864123369, -0.2512526519528831, -0.2889728732178185, -0.2971087011165692, -0.2760576603374701, -0.2005473371067773, 0.06417039953514271, -0.1764755883174434, -0.2216102503298889, -0.07928038664688938, 0.11867425722544883, 0.4372222642259843, -0.17313740755165194, -0.019821485918420936, -0.03667104856809029, -0.3271786197684698, -0.2820393154261455, 0.07833605726866237, -0.20603938001402028, -0.23529018470866153, -0.308944512023817, -0.27613539359114203, -0.08925479258544256, -0.06194536272016994, -0.08176653859127271, -0.060279767886157604, -0.11128611515688017, -0.2987146421185995, -0.2162406481369916, -0.34311695894915284, -0.30648255324942686, 0.07947735643659667, -0.2015383265195761, -0.19824339821765646, -0.22031665363992717, -0.28038622420938897, -0.14900719752184363, -0.3675408116276687, -0.3057474371021281, -0.31911588139532193, 0.07369893373011101, 0.06591046345584464, -0.027567279274626915, -0.19953765867677767, -0.3333243444200504, -0.28250855271467235, -0.08150678679375269, -0.20478568465444527, -0.022495220151094494, -0.2740354866434913, -0.3652825195182057, -0.23913647108260455, -0.25500537393952155, -0.25051841759763305, -0.21836774936164643, -0.14080328197221845, -0.269402220370023, -0.19476122205531157, -0.20005987081365892, -0.3100963712094589, -0.33942960988850024, -0.29835882118322893, -0.05312585294401722, -0.2297827909340919, -0.13804665077254205, -0.3689270709602817, -0.11862570211603263, -0.23164385548887223, -0.12518054833487904, -0.19470145439904793, -0.1803340754874504, -0.15351726119107567, -0.08655032679421935, -0.09668828326994001, -0.3828737066941251, 0.009088584272481735, -0.25499687176981306, -0.3052946626996916, -0.21313202580858373, -0.361436453823195, -0.22297202440220812, -0.2724171880812715, -0.2606481322156049, -0.23909440273089128, -0.19629684212602055, -0.0876720645019289, 0.29431170907312426, -0.28843832184597257, 0.34160227798586834, -0.14729084064596232, -0.36137232968645966, -0.2359223857712264, -0.2380384321811395, -0.1588654137947578, -0.03332382222057985, -0.26971301390477903, -0.33670108226195816], "y": [-0.08446202233858674, -0.04533704653907129, 0.02357982912118513, -0.05086503705014221, -0.006658650288606184, -0.05437266393208491, -0.0005064886068306059, 0.04511384962629367, -0.06985957900910267, -0.13660484511283513, -0.15274558316451267, 0.07896800193439645, 0.08835438168966302, 0.10853683927381451, 0.2298554087848991, 0.08791857287072381, 0.05979519407618659, -0.0370835865960936, -0.026114423041604235, -0.02144725363542858, -0.036357189609442384, -0.023429443755873105, -0.04068832118029075, -0.026364470160085154, 0.004141156917442136, -0.09600754814977822, 0.08206989637386805, 0.052024585593014906, 0.06568821116998672, -0.08142026879201318, -0.01485671159277418, -0.037390852419842545, -0.009447905099448456, 0.007663835886799324, -0.14892591457760937, 0.03689525145795133, -0.05469114836803101, -0.09492202095342571, -0.005026176516432761, -0.11452458377847916, 0.0264921064843988, 0.06712139852796295, 0.11515694618946307, 0.014416153036742974, 0.022571374455391115, 0.0705186457943871, 0.03728865313009817, 0.06503906105764386, -0.0178151003034567, -0.018714325966994325, -0.004445083982314406, 0.04700656509331474, -0.07869190044129201, -0.004253204031816815, 0.046970355387920645, 0.001198440205421033, -0.00036448755601392894, 0.029545788509061144, -0.04398425893299921, -0.09958883150988886, 0.06766811387174419, -0.06011734960824941, 0.002507987856931874, 0.0430991145336332, -0.03287257500888081, 0.0007389366189315021, -0.050007391156140606, 0.003241001794942286, 0.02426786239116364, -0.03507399286968065, -0.04548348788872188, -0.007701256507497726, -0.022104249582776674, 0.009644923536842712, -0.020682263452304058, -0.02739525113067811, -0.031230252197836147, 0.010364397000158402, 0.06036926395843422, -0.06557402458754802, -0.08123950725242836, -0.046208078651984744, 0.007125824910564938, 0.09316909482845132, -0.017638888352927578, 0.044523769223747976, -0.07932286477427085, -0.10741210874090357, -0.12288287117076438, -0.12462565383376116, 0.010628642894445497, 0.020008721575213807, -0.14932021199511583, 0.012616820915679798, -0.018429753491797766, -0.04944695662095026, -0.13931487827816125, -0.10949304162524662, 0.00724320936584177, 0.008247092917819466], "z": [-0.016400164805455503, -0.051790127164220254, -0.028607245731492474, -0.05382350432399864, -0.02402602558146807, -0.03059292970403384, 0.031495363784983584, -0.04509571844515211, 0.00927493366253668, -0.03589731375177537, -0.013915141902817455, -0.025196117699551844, -0.04706534439183589, -0.1382564743611425, -0.1482101406388449, -0.019623503672193815, -0.06255942840827353, -0.03284388857547853, 0.01736305444150553, -0.006786369683817821, -0.09226179545984861, 0.014645555481758473, -0.0244971201446584, 0.003451894553456256, -0.015186125373357133, 0.009960815044259245, -0.07401986681738884, -0.03798596190278194, 0.012080237990747051, -0.01806993104256027, -0.010919891309923225, 0.02945859131821654, 0.016452263818156742, -0.036536169017274966, -0.04231868752202195, -0.060354224523981426, -0.011013747318616741, 0.023529391643848627, 0.011117074401154593, -0.008323147195263293, 0.0063835717907613845, -0.03442491003329405, -0.04836017119501077, -0.09071767580109931, -0.030060645552555682, 0.010612015566632604, -0.004125039550257927, 0.0060053767187429686, -0.014390133007592643, -0.007095443979230835, -0.0022229616587177577, 0.03218168523173031, 0.014621271405548981, 0.01333793309588052, -0.010111280567392321, 0.003423456769365254, -0.018690465938075117, -0.012842937485977892, 0.0035990096771105064, 0.01071379565141367, -0.061951132226703204, -0.030863417933116605, -0.005322776101086285, -0.02046940005882462, 0.01627082332326658, 0.008682289435653703, 0.019170306066233007, 0.030331529717248038, 0.013027904690002964, -0.058427666241939624, 0.014150463254755255, 0.03233146088045739, 0.0011908459862674259, -0.054128884090640615, 0.011164977445764746, 0.04409427088835617, 0.00012293009982280529, 0.034286510379071905, 0.0022934455948204397, -0.002828405776641328, 0.01355478510910905, 0.01547678570454933, 0.010129595494210967, -0.026867491083075106, -0.005845777228972978, -0.026023061285039187, -0.03158381890801973, -0.007038441882817764, 0.06574746173977405, 0.013115781970588866, 0.022200909094510696, -0.002003218335778404, 0.021040532496897, 0.0020458941536323216, -0.0428194319025388, -0.005176013276979716, 0.022170589427888847, -0.021126349117533513, -0.013256391645981932, -0.009944735481378312], "mode": "markers", "marker": {"size": 5, "line": {"width": 1}, "color": 0, "colorscale": "Jet", "opacity": 0.8}, "name": "Normal", "text": [209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308], "hoverinfo": "text+name"}, {"type": "scatter3d", "x": [-0.31505557961176617, 0.01900256256615716, 0.04206234698002762, -0.2791207323152547, -0.2837378591856042, -0.1836097240545254, -0.29407172859710456, -0.21949423382432628, -0.31796674251887375, -0.25322943014355526, -0.2946754659035871, -0.14358003207141634, -0.14470114569294018, -0.023401990860114054, -0.25291064191041746, -0.011403316317820402, -0.3571260682251892, -0.27831316752216306, -0.2836931085097842, -0.2332753587698284, -0.10043862539920213, 0.05400611630486015, -0.2675154520295687, -0.20394951644323384, -0.05813196344011421, -0.35953042343819797, -0.1778302778044053, -0.18709315832035048, 0.04437527672456594, -0.17662782191036253, -0.2692513883757372, -0.22839982443314943, -0.25670401133973214, 0.10423088623281748, -0.34605358105273165, -0.3261527592343601, -0.2586806642271469, 0.03892353370342417, -0.16072119238087906, -0.3330685929645761, -0.30047714888331767, -0.16021682475762222, 0.055020793413259675, -0.06945511377138372, -0.07758024846104584, -0.13496462953490498, -0.24720261099746413, -0.22438147757854454, -0.25614684927170867, -0.1369173306405777, 0.2808153053130204, -0.02576582770407604, -0.2977138408730234, -0.23614199236450115, -0.2326720692787854, -0.31509907483611826, -0.21448048960956728, -0.1954428866545777, -0.21747634409007638, 0.4584713622249646, 0.6735153092027121, -0.21162495717943544, 0.1672858849945071, 0.26905458390278336, 0.5820981674578001, 0.15999133184996803, 0.15799176708307966, 0.00018417951802406028, -0.2400236605167083, 0.012666040678528718, 0.41028404711548216, 0.454732747304303, -0.0774620143400521, 0.3618439444815971, 0.6009878667851697, 0.6309482746473631, -0.150261063553179, 0.02522266454908356, -0.22545359984327082, 0.3968485395633326, 0.14025767758378496, 0.3021193114716282, 0.3742540137478278, 0.5335679039896316, -0.23458049289822624, -0.02518783746555867, -0.2231138115728667, -0.12852213533428644, 0.052772136626412, 0.2871771237058405, 0.09983919614571736, 0.2240145185921111, 0.05876304492182958, 0.23750119127343672, 0.2737670028465, 0.49631793570315835, 0.24239073927847124, 0.417882697061633, -0.14928191058719925, 0.3816823272398575, 0.14599333234362166, -0.010787115286785274, -0.21772694129728754, 0.3587378859472143, 0.09178642347272935, -0.08048252938638058, 0.5219584123380003, -0.10818693012587097, 0.0207189440903068, -0.026444085679523322, 0.5463989541705003, -0.27439285504401423, 0.020928646607490893, 0.7122940105228116, 0.658831284816381, 0.00937739150447077, 0.483828696221105, 0.02411912368977188, -0.11073701567447693, -0.1788033798240183, 0.4475117806886352, 0.6810507290044618, -0.07168370825085066, -0.09398113211411217, -0.2103297352442908, 0.09686885878088837, 0.15863248977774985, 0.4533135503301682, 0.027692150975479817, -0.2597794962828761, -0.0326416406590146, 0.070783196479026, 0.2397744544330795, 0.05380685949180856, 0.46252673250238063, 0.7722097956750141, 0.5707165230134789, 0.30972877761078854, 0.4132265822728731, 0.08611323248206981, 1.000362277935714, 0.4555302241506585, 0.11483177100103309, -0.0678269788807633, 0.7777817944436248, 0.5264515907725245, -0.2572170277251369, -0.11514907287531595, 0.09826640410934097, 0.17033259796679628, -0.21814060918234368, -0.09637410208939913, -0.042761354169375675, -0.2817465150779776, 0.05432618652412886, 0.3438373317883563, -0.2827471386469103, -0.2330523028986156, -0.007159858987742267, 0.6725412505777179, -0.12908249054218446, 0.812654917213852, 0.9859719717867771, -0.21213973360652272, 0.3129242125120133, -0.1615889971494266, 0.02125603877095059, 0.6222179107660019, -0.3178133670525541, -0.035597741356728205, 0.14086214159193797, -0.16066659734368618, -0.1793538484658179, -0.006877077094317669, -0.15107475284996022, 0.00010069432871764589, 0.15450421858865998, 0.27377840789353464, 0.7194341557630815, -0.33498487065449967, 0.017369500866834735, 0.5380629072783916, 0.6026194430946646, 0.3766402376316579, 0.4913579655909345, 0.30205334427805997, -0.15469117645942634, 0.7354085355396134, 0.4616638360883085, -0.20279310174080514, 0.6062369240527604, 0.4672784168312692, 0.03640912095712823, 0.11537557240492784, 0.04123626592989462, -0.010637639179073352, 0.921739172548881, 0.03553855260447953, 0.06717913037495268, 0.08175695090411407, 0.5982337321496292, 0.861807900887561, -0.027885376691224498, -0.14320130313574894, 0.6396721254221989, 0.8459756089687683, 0.2156755675256959, 0.4765101998688292, -0.1848078236289201], "y": [-0.04015858380392344, -0.07634784446780261, -0.12504374091585402, -0.02352421504828963, -0.07441736316364872, -0.058315018546186634, -0.04442449352680436, -0.023756877065239587, 0.02372432798419289, -0.05554072190855283, -0.1222875752402625, -0.09993805716744011, -0.1227887614529828, -0.1275654744769773, -0.036500239752291226, -0.03831430081628658, 0.010414500187943512, -0.060997438174141244, -0.05206138841669867, -0.047206808349909875, -0.10576931762995259, -0.08760819859422206, -0.05916417238335734, -0.1726609080175455, -0.20801572195400367, -0.05934242195055637, -0.042690752173690595, -0.16219506655303248, -0.12101906407678267, -0.0372430561532304, -0.03082645250724485, -0.10361754676832363, -0.036013637579648206, -0.2572524464255425, -0.02656431348113901, -0.0014907812654241805, -0.14039376404153417, -0.20097624589876129, -0.1014238825901486, -0.06680084323458281, -0.018411932052320095, -0.10884539054416045, -0.15067951986874065, 0.03172197626546534, -0.1226165822168969, -0.14199450651636436, -0.0951682833248028, -0.10252516157262866, -0.12264018371110852, -0.1208544780392307, -0.5019252758677709, -0.2561917206311928, -0.038998270498221015, -0.10041977007823524, -0.038702580229757186, -0.013821720392130959, -0.05902368794851439, -0.11981583911136637, -0.056475550592872965, -0.020659402431198005, 0.07334215620762793, 0.05247389355912125, 0.047648704632761864, 0.14268989790322137, -0.3191882791579425, 0.01398140401169943, 0.09919584756203892, -0.019613266278828825, 0.12904039871038722, 0.026508496133079474, -0.15710097503552434, -0.11295980790235004, -0.067021419012027, 0.21101012216628934, -0.19287129311672555, -0.08870161329176311, 0.10949903417194246, 0.0547518340059016, -0.012627958214506175, -0.035534805376074174, 0.0377148894858071, -0.11839156069596252, 0.12235623586810217, -0.36946850009168614, 0.020378338494965436, 0.021972012454406373, 0.026073363141237465, -0.039350840638841415, -0.1024617385298437, -0.08953566478613242, 0.04282256300563976, 0.22320969234631982, 0.18602532388608142, 0.21540225813639363, -0.1803513650334512, 0.35048492540165727, 0.18381330494654305, 0.26482631433395887, -0.003842203893672632, -0.0733451284635105, 0.0905446529143443, 0.10538728250250212, 0.0826038545922307, -0.03710585105196565, -0.1535207150723713, 0.12528963409516627, -0.03719928495276373, 0.16861376610142795, 0.1036735645657421, 0.10349552201913176, 0.0998702882460384, 0.17693340950868205, -0.00545851341917737, -0.01226982537334409, 0.6371775090824817, 0.14405779741529653, -0.30789387891352427, -0.11422860970520349, -0.012946176148527633, 0.012854075934464164, 0.23879037485743287, -0.5160547482673661, 0.11773177599455806, -0.005794024273404414, 0.10577799407054995, -0.0013828955213575766, 0.14733421001949018, 0.03411715786872306, 0.03998769291396655, -0.00019861254442237052, 0.07915008662237089, -0.008464693701516292, 0.13494277138727742, 0.1401970669443871, 0.02834747912313687, -0.11956308201192875, -0.04673778317554778, 0.10434971376847112, 0.2736848378292511, 0.16225005381787297, -0.36070911226047314, 0.4057221077564537, 0.07305512146922846, 0.06156606755040945, -0.2251921149296525, 0.04611340895563899, 0.15202726101805436, 0.08844428196829511, -0.09373051089346388, 0.05496522801971552, 0.12213910747085527, 0.02765661966631092, 0.07669936250114283, 0.04138744011439555, 0.15809099006307004, 0.03541144194356093, 0.08524697141458107, 0.12985909246660318, 0.1503553675799521, -0.04343054108606274, 0.1182083883200907, -0.18989137130945322, 0.01155945697781131, 0.03291261505165844, 0.19394862458988382, 0.11780867934965654, 0.06174028597562681, 0.30376315906397905, 0.052297170666079675, 0.06004912360277332, 0.22459377795395938, 0.0001437877940924466, 0.07844794512738816, -0.1586604743964001, 0.0915981612085154, 0.1315928255342775, 0.08163567585635317, -0.020888720964656396, -0.4908221180005382, 0.01033689072533724, 0.11213682392175593, 0.07093092263706396, -0.08631150672744115, 0.26339381786990973, 0.20165064166239138, 0.16774558784509253, 0.14441173085525238, -0.08902445590031523, 0.04470240592512141, 0.055554413963646264, -0.2746325268942859, -0.07049308166086933, 0.2279691717767625, 0.08672969208582763, 0.33899073575418676, 0.1397178539075143, 0.2844370829349081, 0.12582130283112009, 0.08761836007030042, 0.05658783398192288, 0.09565356351618307, -0.07372734708754372, 0.15947641055507408, -0.0020555962615194353, 0.040352747595309116, -0.4012508059362625, 0.17815246694032746, 0.34300365590144827, -0.05847674339568068], "z": [0.019541338406044818, -0.007450934499048292, 0.03519693858978761, 0.04064675236946421, 0.012919364853756055, 0.014136343362267908, 0.005524556919715199, -0.0165881650235487, -0.020514841026678426, 0.009645752079876078, 0.008327468456517488, -0.0178168821383838, 0.01892984957069647, -0.03411988704285299, -0.0005214614113038839, -0.04133569059220549, 0.00434103046725507, -0.0036035285423209377, 0.003454658186277634, -0.009176552871794665, -0.016724540897090006, -0.03295914262642553, 0.005341152387295207, -0.029259520899832057, -0.009636091719574861, 0.0058463511686829926, -0.04655810514074438, 0.002953659256943073, -0.018547283800900555, -0.005502382186228405, 0.019104541426667062, 0.01411716789012624, -0.02758505680666044, -0.055921016407871775, 0.01385865591350412, -0.00700702099832527, 0.0014157655954668254, -0.06246858997724797, 0.0028760779323661902, 0.018010154937215896, 0.025792108537959543, 0.02147160267788021, -0.015525104684150012, -0.09849875784273651, -0.06189210482316426, -0.012601177627863381, -0.011077743390786748, -0.0221717384221132, 0.01943231923346087, 0.017463629003091372, 0.07539956486307392, -0.029692737696869163, -0.008904689370165907, -0.026730292878881252, 0.005087478153063023, 0.026449757130343074, -0.011101148987467896, -0.025728266253411982, 0.008156970452749612, -0.11040279313317751, 0.010118727777174227, -0.02207806945676196, 0.0072386371731965144, -0.15307435590695007, -0.01837469376980492, 0.035580664471952544, 0.06682452116910162, 0.01935661958657782, 0.03891665623987641, 0.0652984936932491, 0.24101437038354478, 0.10717228589790416, 0.021344641040697943, -0.17390877351941983, 0.04013985217910602, 0.11076754205668775, 0.04665576176575102, 0.0005398658579236496, 0.018423210309782764, 0.0028561289852334195, 0.09690098225623757, 0.18526125787004633, -0.01640392873398288, -0.0031235835867279007, -0.004338485587907318, -0.04293304255683681, 0.014318231513541045, 0.028156652124074713, 0.07223373109816707, 0.047726386016232934, -0.007113517847631803, 0.10206877215425049, -0.12384017210764782, 0.1529773231700868, 0.10414369045894777, 0.017064771973140932, -0.11533049964931087, -0.15821274420290646, 0.05159164957002853, 0.006499260943327917, 0.052483222504443455, 0.01024140740630126, -0.010057355022412146, 0.08460700904631997, -0.031804376621367494, 0.04519379082555013, -0.04129407300015353, 0.011514872088593512, 0.006685114236253006, -0.002982380889836424, -0.04433661516361244, -0.05647350906167203, 0.0890141625623085, -0.11499442219984564, 1.2779599383785099, 0.05055813322053774, 0.15204456369841204, 0.01951545357928355, 0.0517200822518607, 0.055609534746961266, -0.03761204015545005, 0.05512264279050896, 0.057846714089937544, 0.16372163674297402, -0.020869555630563547, 0.00034185858237263054, 0.07091029639433247, 0.08044053291290623, 0.1134287256709244, 0.055313946691298015, 0.01225636428271366, 0.0684776466644201, 0.04066378898982723, -0.13816758548985303, -0.036699419605818566, -0.08517835671648033, -0.07313918712854602, -0.09641488699525172, -0.12463168294848237, -0.07132282100402024, 0.0635051378829128, -0.15413481590056674, 0.03109591574321335, -0.014017252189966946, -0.11497180324685646, -0.008930861699984543, 0.02136391219867855, 0.06522930603508154, 0.1436008262478743, 0.0715931811982015, -0.019682562717867533, 0.04687856617282955, -0.058053674517790074, 0.010684908405088251, -0.04953213360259375, 0.010929032381058409, 0.03110474714926105, 0.07003762296324614, -0.03655706900431308, 0.0043251042341613845, 0.07732197408869843, 0.4402431332806539, 0.2517176927103642, 0.03257854435467467, 0.029856372896347412, 0.007773879618579103, -0.0008838456138146786, -0.06886488332966462, 0.03407216924027462, -0.013253635203959268, -0.08198978549162336, 0.025769689884119498, -0.030611887948016742, 0.033814718291257886, 0.03765480502585562, -0.019906437749823175, 0.07428551072576281, 0.067617062762858, -0.154673260897482, 0.055120150838860085, -0.04885112238910884, -0.19152724168200716, -0.18760674957106432, -0.14802940402048995, -0.016651611202149787, -0.033053594900663445, 0.002925453811831522, -0.09456786707349538, -0.02498806434963672, -0.011667854010853301, 0.05697886543819709, 0.07385166649996813, -0.05821885327270832, -0.034973122241098865, -0.09137104905863588, 0.0013271123138896591, -0.59454130410677, 0.03407931090058646, 0.03375303539761331, -0.06234658032841501, -0.2548461219883878, -0.20847553291899798, 0.02878266169939527, 0.024419401817139183, -0.06693442910788946, 0.15164734143830128, -0.009091020124510798, -0.06046253020407962, 0.02694619519739968], "mode": "markers", "marker": {"size": 5, "line": {"width": 1}, "color": 1, "colorscale": "Jet", "opacity": 0.8}, "name": "Abnormal", "text": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208], "hoverinfo": "text+name"}], {"title": "3D Representation of the patients characteristics using PCA", "hovermode": "closest", "showlegend": true}, {"showLink": true, "linkText": "Export to plot.ly"})});</script>


    The PCA explains the variance of the data by 94.749622%
    

This time, PCA has performed very well, we can see that by the high explained variance ratio that was computed with only 3 components.  
Now what can we see from this ?
* The normal class is somewhat clustered together with a few outlier points
* Some points in the abnormal class are mixed within what seems to be the normal cluster, that shows us overall similarity to the normal points with probably just one factor making them abnormal ( certainly the Degree Spondylolistehsis )
* The abnormal class is very spread out, we can assume that the points the fursthext from all the other poiunts are extreme cases with extreme values.

### TSNE


```python
from sklearn.manifold import TSNE
tsne = TSNE(n_components=3,learning_rate=115.0)
X_tsne = tsne.fit_transform(X, y)


xs = X_tsne[:,0]
ys = X_tsne[:,1]
zs = X_tsne[:,2]

# Recreating the df with the new coordinates
df = pd.DataFrame(dict(x=xs, y=ys, z=zs, Attribute=y)) 
l = []
names = ['Normal','Abnormal']

for i in [0,1]:    
    trace= Scatter3d(
        x= df[df['Attribute']==i]['x'],
        y= df[df['Attribute']==i]['y'],
        z= df[df['Attribute']==i]['z'],
        mode= 'markers',
        marker= dict(size= 5,
                    line= dict(width=1),
                    color= i,
                    colorscale='Jet',
                    opacity= 0.8
                   ),#name= y[i],
        name = names[i],
        text= df[df['Attribute']==i].index,# The hover text goes here...
        hoverinfo = 'text+name'
    )

    l.append(trace)

layout= Layout(
    title= '3D Representation of the patients characteristics using TSNE',
    hovermode= 'closest',
    showlegend= True)

fig= Figure(data=l, layout=layout)
plotly.offline.iplot(fig)

```


<div id="20e4aace-08a3-4fd6-bde5-481e71a88ce0" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("20e4aace-08a3-4fd6-bde5-481e71a88ce0", [{"type": "scatter3d", "x": [7.791319847106934, -11.619583129882812, 32.50340270996094, -3.4726433753967285, 32.42058181762695, 26.903518676757812, 38.85100173950195, 25.595792770385742, -14.497599601745605, -36.14038848876953, -20.452342987060547, 35.11973190307617, 36.4173698425293, 14.70853042602539, 12.673325538635254, 36.922645568847656, 32.500946044921875, -15.45008659362793, 15.567293167114258, 20.976303100585938, -10.094221115112305, -3.7995264530181885, 19.118980407714844, 16.712539672851562, 15.73489761352539, -31.700183868408203, 39.95255661010742, 34.531227111816406, 22.102174758911133, -18.15157699584961, 18.037952423095703, -13.155280113220215, 10.954150199890137, 29.273944854736328, -33.94951248168945, 30.179492950439453, -3.3194475173950195, -13.713976860046387, 30.521930694580078, -24.04311180114746, 6.323744773864746, 26.0035400390625, 22.169082641601562, 0.5216614603996277, 1.6900849342346191, 22.960262298583984, 16.311809539794922, 20.024351119995117, 30.291414260864258, 49.32835006713867, 7.1878581047058105, 7.963935852050781, 3.6797029972076416, -0.5419836640357971, 28.02495765686035, 16.717754364013672, 23.748825073242188, 23.6084041595459, -9.585596084594727, 5.621099472045898, 39.42844772338867, 6.621890068054199, 23.199399948120117, 16.387638092041016, 22.313629150390625, 19.574243545532227, -1.7998309135437012, 28.05635643005371, 10.485614776611328, -1.5910441875457764, -6.878280162811279, 22.128517150878906, 0.030842941254377365, 32.95762634277344, 13.923187255859375, 25.178678512573242, 11.960264205932617, -2.7895989418029785, 12.845809936523438, 5.181004047393799, 5.978418350219727, -4.060344219207764, 7.093690872192383, 27.216217041015625, 20.798551559448242, 33.04701614379883, 11.446687698364258, -12.822160720825195, -40.598777770996094, -29.143516540527344, 39.50563049316406, -17.298555374145508, -37.50476837158203, 12.74746322631836, 28.470897674560547, 5.547626495361328, -33.69175720214844, -28.361371994018555, 27.792137145996094, 23.707538604736328], "y": [-23.043682098388672, 3.6568756103515625, -37.79048156738281, -5.414748191833496, -38.05083465576172, -44.32843017578125, -70.42481994628906, -49.63892364501953, -30.98379135131836, 20.91689682006836, -21.52656364440918, -28.960933685302734, 17.17523765563965, 47.15986251831055, 79.13652801513672, -17.56824493408203, 15.686931610107422, 1.9787108898162842, -81.96141052246094, -55.507354736328125, 23.221052169799805, -33.642494201660156, -28.85610580444336, -68.84555053710938, -49.074729919433594, -8.786430358886719, 9.42780590057373, 3.311415433883667, 4.774219512939453, 0.2648829519748688, -58.175235748291016, -37.63648223876953, -74.27540588378906, -59.92724609375, 28.814287185668945, -17.633893966674805, -21.098697662353516, -40.0107421875, -56.04323959350586, -19.36103630065918, -75.51881408691406, -60.50800323486328, -60.764522552490234, 26.30980682373047, 17.22336769104004, 8.279540061950684, -31.819372177124023, -77.02001190185547, -47.56868362426758, 25.87434959411621, -31.04606056213379, 4.297032833099365, -57.12165451049805, -76.50785064697266, -32.33386993408203, -43.506595611572266, -35.347442626953125, -27.720300674438477, -10.681556701660156, -53.13478088378906, -21.798513412475586, -19.448461532592773, -66.98188018798828, -67.46818542480469, -69.02043914794922, 3.3049864768981934, -44.2388801574707, -4.583304405212402, -86.41224670410156, 3.4333882331848145, -42.72037124633789, -0.8585882782936096, -23.961055755615234, -17.767559051513672, -7.99363899230957, 8.110488891601562, 5.06640100479126, -80.13056182861328, 14.540322303771973, -45.686302185058594, -69.42231750488281, -32.1655158996582, -85.54070281982422, -29.20942497253418, -46.17765808105469, -43.474308013916016, -33.34663009643555, -27.583425521850586, -12.859167098999023, 41.58677673339844, -60.32383346557617, 47.82006072998047, -16.040054321289062, -77.75984954833984, -28.044952392578125, -43.58753967285156, -26.465599060058594, 4.459207057952881, -47.33884811401367, -73.49739837646484], "z": [-44.143836975097656, -25.367849349975586, 10.26921558380127, -35.42540740966797, -3.303558349609375, -34.53202819824219, -2.18286395072937, 25.72747230529785, -20.422712326049805, -21.36673927307129, -54.37150192260742, 29.938865661621094, 33.42731857299805, -4.306704044342041, 6.413323402404785, 32.62560272216797, 8.831840515136719, -13.949172019958496, -3.6872811317443848, -11.72518253326416, -18.193262100219727, 0.7689726948738098, -18.508548736572266, -5.90406608581543, 15.873451232910156, -17.253002166748047, 3.5933854579925537, 11.643106460571289, 6.685292720794678, -34.76438903808594, -0.6278813481330872, -6.688100337982178, 5.108891487121582, 15.304418563842773, -13.72504711151123, 17.861433029174805, -25.105443954467773, -30.89586639404297, -3.873788356781006, -41.75722122192383, 27.084651947021484, 30.123388290405273, 41.079071044921875, -18.39784049987793, -7.077836513519287, 18.53165626525879, 18.295738220214844, 34.173912048339844, -13.579072952270508, 5.911459922790527, 4.165783882141113, 1.4926677942276, -34.67200469970703, 8.442170143127441, 22.20189094543457, 3.778444766998291, 1.633097529411316, 12.803823471069336, -4.897364616394043, -46.949703216552734, 21.43705177307129, -31.456064224243164, 6.504716873168945, 27.228172302246094, -16.752347946166992, -5.528149127960205, -7.524141311645508, -14.668164253234863, 24.719873428344727, -30.206668853759766, -17.055349349975586, -22.450218200683594, -2.3274929523468018, 7.831606864929199, -14.688039779663086, -15.457839012145996, -17.950916290283203, 19.850421905517578, -1.4206205606460571, -29.113372802734375, -33.74021911621094, -11.979109764099121, 11.714240074157715, 41.529666900634766, -6.795896530151367, 19.411088943481445, -39.35161209106445, -39.12914276123047, -22.607826232910156, 30.329776763916016, 1.6222609281539917, 31.976482391357422, -46.5106086730957, 17.453495025634766, -7.378272533416748, -17.400671005249023, -45.48485565185547, -16.29607582092285, 4.651977062225342, 17.744579315185547], "mode": "markers", "marker": {"size": 5, "line": {"width": 1}, "color": 0, "colorscale": "Jet", "opacity": 0.8}, "name": "Normal", "text": [209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308], "hoverinfo": "text+name"}, {"type": "scatter3d", "x": [16.101642608642578, -22.763317108154297, -37.6086311340332, 36.97816467285156, 12.937697410583496, -13.197783470153809, 10.198713302612305, 13.976806640625, 18.54043197631836, -1.620475172996521, -2.5577855110168457, -16.315324783325195, -36.06285095214844, -34.25468826293945, 129.29393005371094, -12.697779655456543, 5.789247035980225, 16.956947326660156, 17.69845199584961, 11.656753540039062, -26.0566349029541, -24.163196563720703, 8.562294006347656, -12.987236022949219, -45.5687141418457, 0.25579747557640076, 3.1530890464782715, -22.188323974609375, -31.345584869384766, -1.8027938604354858, 29.80548858642578, -9.494782447814941, 24.4796199798584, -44.998836517333984, 4.132210731506348, 22.727340698242188, -5.234711170196533, -48.86278533935547, -23.641841888427734, 6.946709632873535, 28.73476219177246, -27.64468002319336, -43.30400466918945, 46.84852600097656, -30.184432983398438, -24.980615615844727, 2.911627769470215, -1.2460764646530151, -6.944366455078125, -32.061885833740234, -58.51353454589844, -47.243309020996094, 31.047632217407227, 4.679618835449219, 7.903548240661621, 26.355045318603516, 5.265570163726807, -9.124811172485352, -2.3734261989593506, -26.564899444580078, -31.125268936157227, 13.029274940490723, -10.517755508422852, 18.842317581176758, -51.79323196411133, -17.2239990234375, -0.138591006398201, -4.7398552894592285, 18.565292358398438, -6.007480144500732, -40.61062240600586, -36.47806167602539, -26.70632553100586, 10.595949172973633, -45.82419967651367, -39.56156539916992, 13.00583267211914, -34.49283981323242, 10.419546127319336, -25.169612884521484, -4.838852405548096, -33.0705451965332, -33.39421844482422, -58.865966796875, 11.151378631591797, 42.83851623535156, 2.7367188930511475, -16.468849182128906, -33.794097900390625, -22.92329216003418, -0.5685955286026001, -11.540630340576172, 28.42038345336914, -11.385616302490234, -36.39810562133789, -5.34719181060791, 14.705493927001953, 3.9755752086639404, -13.8184232711792, -25.624927520751953, -0.8393635749816895, 16.62761688232422, 18.157390594482422, -28.512615203857422, -44.91606903076172, 21.166011810302734, -32.91523361206055, 25.19644546508789, -33.72960662841797, 25.17259979248047, -17.90947914123535, -21.32737922668457, -16.372455596923828, -39.09591293334961, -25.21798324584961, 12.937231063842773, -49.6361083984375, -37.3796501159668, -15.94914436340332, -7.979767799377441, -7.050039291381836, -64.6738052368164, 13.186055183410645, 38.68596267700195, 15.454085350036621, -9.18281078338623, -3.3706588745117188, -13.927553176879883, -5.319579601287842, 45.8629150390625, 12.961451530456543, -13.222320556640625, -3.5487945079803467, 29.239259719848633, -22.793170928955078, -51.03492736816406, -32.79187774658203, 8.016757011413574, 4.538594722747803, 21.121440887451172, -65.48342895507812, 7.847070217132568, 5.830885410308838, 34.771522521972656, -54.89111328125, -14.391887664794922, 15.491272926330566, 6.544766902923584, -30.63591194152832, -11.954729080200195, 23.39346694946289, -6.974853992462158, 37.471168518066406, 7.567409038543701, 25.134382247924805, 45.94809341430664, 5.281535625457764, 7.267663478851318, 25.248146057128906, -41.518646240234375, 16.06678009033203, -52.97419738769531, -47.742828369140625, 3.2331478595733643, -4.798985958099365, 19.543508529663086, 5.248512268066406, 3.5051023960113525, 3.3024098873138428, -25.995479583740234, 24.772253036499023, -8.44207763671875, 23.42249298095703, -44.15903091430664, 7.779708385467529, 20.312511444091797, -6.429739952087402, -19.69823455810547, -65.7295913696289, -5.095534324645996, 31.76499366760254, -20.27315902709961, -33.953975677490234, 14.167577743530273, -3.5735208988189697, 4.064890384674072, 27.9161434173584, -43.92835998535156, -20.301443099975586, 4.575623512268066, -51.45701599121094, -34.084110260009766, 32.62229537963867, 8.652627944946289, 31.17017936706543, 24.199848175048828, -34.72335433959961, 7.684518814086914, 4.358105182647705, 11.393136024475098, -24.487226486206055, -46.505470275878906, 10.762277603149414, -6.020328998565674, -28.600614547729492, -58.5418701171875, 3.7340338230133057, -3.16070294380188, -13.42509651184082], "y": [-78.77294158935547, 12.142568588256836, 19.83935546875, -67.63520050048828, -61.4581184387207, -24.290054321289062, -62.378662109375, -22.32074737548828, -62.73238754272461, -53.90314483642578, -58.771541595458984, -12.163418769836426, -21.511951446533203, 8.598142623901367, -14.19199275970459, 11.355301856994629, -68.93067932128906, -51.743228912353516, -60.91679763793945, -33.98475646972656, -4.414407253265381, 21.544204711914062, -54.363800048828125, -31.55384063720703, 3.1058003902435303, -78.07003021240234, -9.699718475341797, -32.29650115966797, 16.968534469604492, -16.93985366821289, -59.354759216308594, -40.23564147949219, -38.10171890258789, 26.887266159057617, -81.5000228881836, -79.54335021972656, -47.01587677001953, 17.14042854309082, -19.375019073486328, -76.45509338378906, -67.3482894897461, -29.092273712158203, 17.538606643676758, 6.4061126708984375, 5.7567033767700195, -10.06295394897461, -41.358924865722656, -29.939891815185547, -51.77848434448242, -11.6583251953125, 40.680660247802734, 12.5828857421875, -54.556732177734375, -34.70841979980469, -34.70256423950195, -78.22486114501953, -25.935211181640625, -22.896392822265625, -33.49061965942383, 70.32862854003906, 77.10228729248047, -22.978105545043945, 36.53519058227539, 66.09529113769531, 61.63285827636719, 34.09577560424805, 45.46392059326172, 7.122712135314941, -39.8402214050293, 12.660595893859863, 42.3663330078125, 51.22964859008789, -5.166391849517822, 72.2777328491211, 56.05375671386719, 63.14318084716797, -1.9881340265274048, -16.82122230529785, -17.8178653717041, 57.11770248413086, 36.78263473510742, 37.246734619140625, -63.335391998291016, 54.06797409057617, -19.43876838684082, 17.458580017089844, -19.862794876098633, -10.799525260925293, 11.89761734008789, 39.332115173339844, 29.156728744506836, 54.839073181152344, 43.65163040161133, 51.19289016723633, 46.87261962890625, 89.2788314819336, 60.199947357177734, 80.73782348632812, -15.571771621704102, 50.6420783996582, 42.558799743652344, 16.35024642944336, -31.880558013916016, 49.083091735839844, 27.974994659423828, 9.825145721435547, 63.878456115722656, 5.072212219238281, -23.100826263427734, 19.967086791992188, 77.03410339355469, -54.929256439208984, 15.865497589111328, 86.74797821044922, 61.48033905029297, 26.09588050842285, 49.260887145996094, 7.933315277099609, -4.776831150054932, -24.554832458496094, 76.77818298339844, 59.28450012207031, 12.87636661529541, 7.716129302978516, -24.210771560668945, 23.9634952545166, 45.58041763305664, 57.0980224609375, 16.54481315612793, -58.198936462402344, 5.455024719238281, 25.04458236694336, 54.95867156982422, 47.522987365722656, 64.33055114746094, 81.30147552490234, 74.76350402832031, 62.1864128112793, 78.79176330566406, 37.86711120605469, 75.02227020263672, 90.54605865478516, 42.68201446533203, 10.77783203125, 74.70431518554688, 68.28184509277344, -34.40936279296875, 2.9562108516693115, 20.665891647338867, 45.006446838378906, -24.713134765625, -3.4725687503814697, 23.884981155395508, -47.5168342590332, 30.50999641418457, 59.09912109375, -45.658775329589844, -37.286251068115234, 33.96937942504883, 74.64999389648438, -0.9906745553016663, 69.86143493652344, 77.92793273925781, -28.994888305664062, 64.61196899414062, -11.847956657409668, 15.481376647949219, 88.35504150390625, -57.22551727294922, -28.796459197998047, 49.669769287109375, -20.647680282592773, -19.144559860229492, 4.7778639793396, -9.790823936462402, 27.114198684692383, 36.6898193359375, 43.09572982788086, 66.45756530761719, -63.8493766784668, 29.351469039916992, 81.21969604492188, 86.19953918457031, 78.9993896484375, 77.374267578125, 64.7431869506836, -5.992287635803223, 77.32024383544922, 67.251953125, -24.939626693725586, 60.901893615722656, 56.89519119262695, 36.80539321899414, 35.027244567871094, 37.00953674316406, 28.756689071655273, 100.53841400146484, 24.098329544067383, 27.88469123840332, 29.549272537231445, 92.87405395507812, 90.7993392944336, 14.217891693115234, -10.634008407592773, 83.66898345947266, 67.07581329345703, 55.376583099365234, 88.17091369628906, -15.442553520202637], "z": [-14.803340911865234, -5.436358451843262, -2.0374059677124023, -15.553674697875977, -38.42523193359375, -10.456648826599121, -14.076064109802246, -8.93689250946045, 17.01850700378418, -17.358474731445312, -53.742042541503906, -38.97358322143555, -34.85988235473633, -24.345212936401367, 15.219813346862793, -12.196125984191895, 15.520000457763672, -33.359127044677734, -26.00855827331543, -24.806324005126953, -29.46382713317871, -19.228622436523438, -23.092918395996094, -58.85774230957031, -30.1492977142334, -15.080204963684082, -27.405792236328125, -51.119224548339844, -11.371508598327637, -13.252007484436035, -20.00071144104004, -41.25774002075195, -23.178539276123047, -29.535940170288086, -2.5395867824554443, 8.264867782592773, -54.8771858215332, -23.823650360107422, -30.559921264648438, -23.86678695678711, -7.225611209869385, -34.74924087524414, -11.8983793258667, 12.375312805175781, -35.207191467285156, -48.92269515991211, -41.076236724853516, -41.89393615722656, -44.36735153198242, -37.121219635009766, 15.805877685546875, -34.98469924926758, -35.74578857421875, -50.09156799316406, -11.17440414428711, -5.5373663902282715, -20.0383358001709, -48.459556579589844, -23.522130966186523, 11.854267120361328, -0.6581522226333618, 31.93272590637207, -6.621622085571289, -2.7952136993408203, 22.847862243652344, 1.59129798412323, 11.503212928771973, -1.6797549724578857, 49.334075927734375, 12.45435905456543, 8.811589241027832, 15.144004821777344, 6.112497329711914, -16.238805770874023, -2.2879998683929443, 7.760578155517578, 48.9096565246582, 18.23006248474121, 6.716494083404541, 12.484834671020508, 18.351581573486328, 17.26209831237793, -26.92959976196289, 18.90793228149414, 19.405532836914062, 16.897655487060547, 28.217370986938477, 6.086069583892822, 4.572005271911621, 22.48983383178711, -5.026358604431152, -13.109075546264648, 9.052982330322266, -23.465286254882812, -6.592296123504639, 2.784670829772949, -10.022272109985352, -13.929523468017578, 18.400808334350586, 24.931913375854492, -0.8821113705635071, 12.7412691116333, 36.79819869995117, 5.360826015472412, -16.041637420654297, 35.52052307128906, 19.36177635192871, 45.95965576171875, 7.1859588623046875, 24.93744468688965, 5.089338302612305, 20.934280395507812, 17.02657127380371, 10.065510749816895, -18.999391555786133, 29.627073287963867, 13.917962074279785, -6.653723239898682, 16.343942642211914, 22.29353904724121, -5.964138031005859, 9.911577224731445, 41.28322982788086, -21.297481536865234, 44.913841247558594, -1.6862226724624634, -11.753772735595703, 20.20783042907715, 21.838947296142578, -15.79939079284668, 13.381465911865234, 14.706700325012207, -3.8110904693603516, 18.03664207458496, 3.342024564743042, 8.523300170898438, 19.781522750854492, 4.368953704833984, -1.775031328201294, -2.2879655361175537, 7.482996940612793, -4.372333526611328, 24.401979446411133, 24.6308536529541, 17.49205780029297, 12.010942459106445, 57.36564636230469, 37.19132614135742, 10.665244102478027, 1.342254638671875, 51.71992111206055, 24.064289093017578, 25.088672637939453, 26.73598861694336, 4.053833961486816, -7.767998695373535, 37.53126525878906, 49.18262481689453, 22.208572387695312, 5.095710754394531, 36.48527908325195, -9.852685928344727, -8.06116771697998, 19.03952980041504, -12.746265411376953, 47.35981750488281, 5.953739643096924, 9.693761825561523, 29.887821197509766, 19.77045249938965, -10.09704303741455, 9.486422538757324, 36.26725769042969, -16.557552337646484, 44.45048904418945, 15.362274169921875, 5.704489231109619, 13.054826736450195, 19.033601760864258, 23.920989990234375, 13.383264541625977, 18.27350616455078, 22.239044189453125, -6.850934982299805, 7.230230331420898, -6.291995525360107, 43.35300827026367, 16.65464210510254, 21.570140838623047, 38.20387649536133, 12.310803413391113, -1.332540512084961, -3.6798295974731445, -3.6555025577545166, -14.747364044189453, 32.92759704589844, 16.096860885620117, 16.68812370300293, 6.741947650909424, -14.696823120117188, 16.927978515625, 19.100818634033203, 23.65088653564453, 12.081239700317383, 8.677899360656738, 2.9949088096618652, -15.490912437438965, -8.162922859191895, -19.914505004882812], "mode": "markers", "marker": {"size": 5, "line": {"width": 1}, "color": 1, "colorscale": "Jet", "opacity": 0.8}, "name": "Abnormal", "text": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208], "hoverinfo": "text+name"}], {"title": "3D Representation of the patients characteristics using TSNE", "hovermode": "closest", "showlegend": true}, {"showLink": true, "linkText": "Export to plot.ly"})});</script>


I always liked the results of a t-SNE dimensionality reduction...  
Compared to the results we've had for the PCA, the separation between clusters makes itself clearer. Yet we still have an important mix of classes in what should be the normal cluster. It is interesting to note that the outlier points in this representation are not the same as the outlier points we've had using the PCA. Different techniques different results, but it would still be interesting to compare these outlier points wioth the rest of the data set.
### Spectral Embedding


```python
from sklearn.manifold import SpectralEmbedding

SE = SpectralEmbedding(n_components=3)
X_SE = SE.fit_transform(X, y)


xs = X_SE[:,0]
ys = X_SE[:,1]
zs = X_SE[:,2]

# Recreating the df with the new coordinates
df = pd.DataFrame(dict(x=xs, y=ys, z=zs, Attribute=y)) 
l = []
names = ['Normal','Abnormal']

for i in [0,1]:    
    trace= Scatter3d(
        x= df[df['Attribute']==i]['x'],
        y= df[df['Attribute']==i]['y'],
        z= df[df['Attribute']==i]['z'],
        mode= 'markers',
        marker= dict(size= 5,
                    line= dict(width=1),
                    color= i,
                    colorscale='Jet',
                    opacity= 0.8
                   ),#name= y[i],
        name = names[i],
        text= df[df['Attribute']==i].index,# The hover text goes here...
        hoverinfo = 'text+name'
    )

    l.append(trace)

layout= Layout(
    title= '3D Representation of the patients characteristics using Spectral Embedding',
    hovermode= 'closest',
    showlegend= True)

fig= Figure(data=l, layout=layout)
plotly.offline.iplot(fig)

```


<div id="f7fa1105-6dad-422a-8781-369c7dc09d9f" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("f7fa1105-6dad-422a-8781-369c7dc09d9f", [{"type": "scatter3d", "x": [-0.2856903416217989, -0.027652845341015617, -0.3566187441064693, -0.15130428409179758, -0.3729887923034235, -0.26657752700087384, -0.3779858623431849, -0.3027767027952524, -0.3374858084088099, 0.08709479395312815, -0.1699527269475779, -0.21445469950071197, 0.06581839520235176, 0.20321381844450903, 0.32639683379346707, -0.16537493538494236, 0.16722717868996526, 0.07631742420386566, -0.40847964623605976, -0.49749938902350327, 0.1378480486350759, -0.2983916300182199, -0.3909850742891132, -0.4588924943184127, -0.35747144673655074, -0.02110151438845894, 0.11000228283682145, 0.0525484163313064, 0.10977107033038754, -0.08948097836979439, -0.49258181431787307, -0.29353117010903595, -0.3739157839271626, -0.3680468904829729, 0.09690264303753893, -0.2234191539788788, -0.33803364359466087, -0.2871183678517418, -0.4886830172749115, -0.1555227027355457, -0.30767132831999994, -0.29934188677117846, -0.18766129418388935, 0.1763903889465694, 0.20031274660702447, 0.1772519970418282, -0.27116307186171373, -0.25550060173483985, -0.4921835851875483, 0.008947851074648273, -0.2651881544455648, 0.09409063587243914, -0.33434243628651805, -0.3291381698402611, -0.26299916742338747, -0.45149615990725706, -0.39619206984478195, -0.28683154732457034, -0.15720230277305206, -0.260673964846252, -0.19542282866914207, -0.25644991357134334, -0.48793142410306845, -0.3230981406209715, -0.4715901954150039, 0.07634755103015642, -0.30900354212688647, -0.07932680057258903, -0.30732865433955775, -0.0877642260070959, -0.37436310374482573, -0.07737394066393842, -0.28025566878197944, -0.20842777764008874, -0.15423042292991987, 0.006996067112291407, -0.04418282120604697, -0.2834378198753036, 0.17269780229718268, -0.3843527098781637, -0.2777013803718977, -0.3245485707474342, -0.3287278682904496, -0.24325573572089237, -0.4273358619249123, -0.3506950846735552, -0.2969994610674416, -0.24823228412539586, -0.02699697640773032, 0.2578596971451249, -0.3948580929532301, 0.38162723739762255, -0.1273433345009894, -0.3338323430157348, -0.278707944996516, -0.38133753828936967, -0.1422244981283887, 0.035295511882931355, -0.4458461188070888, -0.400558509354273], "y": [-0.051659849251118495, 0.3597902769565317, -0.18717611081653138, 0.14916141166511385, -0.19701580639993474, -0.19169815766973444, -0.3540433301878952, -0.18864252860042022, 0.08150746953039378, 0.34194632616059883, 0.11622118867686136, -0.04627821827219373, 0.40894883399134757, 0.22072460967175184, -0.1981076781019718, 0.09672418040627578, 0.5469166744495865, 0.554677710113348, -0.4123847695243302, -0.40647613677108874, 0.34117596938062295, 0.004371971445931316, -0.1340171166975857, -0.43944078743637593, -0.2727287724167191, 0.39431919705555396, 0.4358058926465499, 0.4657459102563185, 0.5418487005366659, 0.38753979522688153, -0.4472043329051926, -0.020395472281726792, -0.3940101519029441, -0.34187927232645815, 0.3324249045245451, -0.028031977714815234, 0.06071112100642249, 0.013055956127576688, -0.4206404266020242, 0.22937299576128156, -0.32695748915731837, -0.2357427844040091, -0.13494060696122454, 0.3952207625554497, 0.39967756687225947, 0.6209378495956013, 0.015243213884297202, -0.24644735039590535, -0.40776009937880225, 0.45676463231507886, 0.010991739075833932, 0.36619396968630613, -0.17923930046899095, -0.3513240447917064, -0.10012167667176447, -0.26849983624904916, -0.20560883698733365, -0.055995389430029484, 0.295270321752636, -0.10551744266063133, -0.004538005287623294, 0.032293905667952295, -0.47070296029051334, -0.3210486230455982, -0.4311417652012245, 0.5171600206168516, -0.06630462078091691, 0.21966506318977455, -0.3253640580052931, 0.21677677428072886, -0.09960929168555482, 0.2767977760114489, 0.04445817458112857, 0.04650110356347558, 0.2213324928790457, 0.3712797789627224, 0.3944811509985646, -0.30459230912670293, 0.4397199547817176, -0.15905916435105638, -0.21508587974580967, 0.017630281987673555, -0.35115350533374223, -0.04442911878450249, -0.3070926296987074, -0.19331054777765164, -0.05963336268826271, 0.10348852488889596, 0.2859723016846498, -0.009246447941812183, -0.35464768320608353, -0.1679887713278225, 0.1937027057288984, -0.35329546030324893, -0.09698034282033449, -0.12645048721985291, 0.1713329052720339, 0.40807214342110504, -0.3309174537529709, -0.4101397675278666], "z": [0.4533910301436726, 0.28501429622177493, -0.2705767597885763, 0.39033829591766517, -0.048994364440854865, 0.04005307823082991, -0.27952581849555386, -0.31754557794873695, 0.6927086048441263, 0.25358155638204816, 0.5650911541746265, -0.266253241160595, -0.42609499618539315, -0.23373984466784165, -0.013399089364648417, -0.2701768067422711, -0.37101608719801593, 0.1482105934048284, -0.33002589502927, -0.18975691692146207, 0.014312704753592451, 0.2641946748930944, 0.2899554588026838, -0.3124203571722073, -0.2406361666960033, 0.41794829794866406, -0.31659502932883676, -0.22163819122813988, -0.3026286046568459, 0.5707537545296613, -0.3400064816692246, 0.30700496301227137, -0.37551886884366636, -0.3341664891774061, 0.24030757098666455, -0.14917362356481711, 0.6219051394857267, 0.582645280547333, -0.32776476592248843, 0.6463357308146063, -0.3504308628897072, -0.361261983390415, -0.2608242456478069, -0.18582732747776448, -0.1653384218111192, -0.4663546045722883, -0.1784113952575427, -0.30540483869635776, -0.2356091834698143, 0.1401824897644312, 0.14403282505690418, -0.159490348834888, 0.24538451048788737, -0.3485495072152587, -0.22592526268859683, -0.15628674620834768, -0.10312644552108235, -0.1480725180852533, 0.4943371997428676, 0.29953089851808085, -0.23973758635121709, 0.5061303653731719, -0.4391330925792967, -0.38119980637169176, -0.2508571001010365, -0.032732450437034925, 0.3273897396190721, 0.11153057866433472, -0.34552697183858616, 0.274038012302073, 0.33605493220398946, 0.1721648103673265, 0.27250265138428825, 0.052823224345889055, 0.29736069742833676, 0.1204918944802079, 0.2504626314096649, -0.31224740054701045, -0.2571237836277084, 0.36136586462624315, 0.024060158112941894, 0.4540838436017268, -0.36098674171076733, -0.35402601789320665, -0.0927745828796524, -0.36198715822782984, 0.44750034258737226, 0.6839550020671087, 0.3763963883790855, 0.08057411088100616, -0.32492662948379814, -0.01276692717053737, 0.5665079485369566, -0.3670985312153518, 0.07407607386553887, 0.34583041065984116, 0.5678085610731689, 0.3526614886535537, -0.31163479168999686, -0.4224250196952366], "mode": "markers", "marker": {"size": 5, "line": {"width": 1}, "color": 0, "colorscale": "Jet", "opacity": 0.8}, "name": "Normal", "text": [209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308], "hoverinfo": "text+name"}, {"type": "scatter3d", "x": [-0.4326467355713903, 0.10942933426749381, 0.1044726466407719, -0.33825985714304657, -0.33670991149183993, -0.27464866220487294, -0.36415546870647886, -0.3355867738960748, -0.40393514877398834, -0.356161537037391, -0.19218680558829324, -0.16216857867099999, -0.14429976445973264, 0.029404610470460724, -0.4596656871210951, 0.10081935609637568, -0.3338092765987859, -0.3872602653909454, -0.4357896218304665, -0.4051474746644443, -0.08422765239742164, 0.11916668986179345, -0.4321785553264237, -0.14859212160655577, -0.01223243859455586, -0.2795073589504563, -0.17231199900934063, -0.17021655414339606, 0.08822898414454582, -0.260252898994154, -0.46710376985977525, -0.2884380651564946, -0.3619500574458452, 0.063124463388823, -0.35411491408347645, -0.41157350186131103, -0.19639417424045646, 0.04169445399549343, -0.18903523740552164, -0.3011704128889961, -0.4434578437184131, -0.17149980655874936, 0.09217259651018762, 0.03328027274034734, -0.04991521707940801, -0.1342632171753929, -0.3012224214242974, -0.25546598289882644, -0.23365327508885952, -0.13437198997869365, 0.17573127193807964, 0.00026728233305487945, -0.29018061848411486, -0.2544313007379705, -0.4305166750583089, -0.4376846518575237, -0.37501288783744574, -0.21753170277970538, -0.36137380993565454, 0.6449480428058688, 0.5638383584699537, -0.2579067470026421, 0.3377794505816993, 0.3477778437691439, 0.43908084955658644, 0.30748620374200025, 0.28681052516072775, 0.13448634888566474, -0.18760396293504844, 0.18757046522032125, 0.3914455100495533, 0.5953588456753375, -0.06713925566274921, 0.4091361270222356, 0.4148376904950905, 0.5453785634431254, -0.1233460910805951, 0.2592003790428494, -0.307164501782585, 0.6016490030980758, 0.2605664752812457, 0.3207625219299007, 0.5973537870579417, 0.41000950689149235, -0.3137464748936186, 0.140558217299404, -0.21911300149303214, -0.14618763815726907, 0.13606065997256173, 0.3517930069632871, 0.30483849908880956, 0.3092212759344309, 0.14791135627163987, 0.22966494038413424, 0.19502419671853913, 0.31737740096689104, 0.3636402501252814, 0.4194114332910747, -0.1387386066660236, 0.5268930299374874, 0.3130091515863883, 0.20622310591404497, -0.2587436324935684, 0.46705801384943346, 0.10710609404436579, 0.05025030625701547, 0.6091775751240304, -0.010885400543189941, 0.2639049730136625, 0.1562355887983763, 0.6191382790353378, -0.1454777547292287, 0.11772036009791592, 0.5270231003755064, 0.21759966255748578, 0.1854196228422169, 0.41160117273366076, 0.07851104660118584, -0.09233222934461688, -0.17370491774589927, 0.489456720130674, 0.356332406866362, 0.0494479196662757, 0.0013199951001759158, -0.20815602915041323, 0.2515324430413789, 0.2883703050868428, 0.6001650367236668, 0.17693186533575786, -0.252526275487009, 0.14863106633533357, 0.19913036073903861, 0.44164118956144455, 0.10447035988781765, 0.6614648011501687, 0.4665320619510698, 0.5984720434914753, 0.5296997931896291, 0.39153429994511296, 0.2642901763455077, 0.3324694816633685, 0.31425451872223137, 0.12518030041218842, 0.08642639409574078, 0.44870797253874184, 0.7058190591430114, -0.17018750046370879, -0.032405025359172175, 0.17177119456749476, 0.3076713585492543, -0.20164848878111521, -0.01321035419349721, 0.11878575409418561, -0.2963314815907562, 0.20623479565346797, 0.5999181356798353, -0.22725265313532514, -0.15491724214672536, 0.15182820285886067, 0.5662522710390933, -0.056379885668076515, 0.2835175146316002, 0.2902098305712388, -0.2710808671910405, 0.4551854666717746, -0.14833834292530687, 0.22788152685174848, 0.38754542025736516, -0.31135728440875243, 0.1641889577684043, 0.26108312536278605, -0.20705187104811812, -0.1994690305823667, 0.0379728459190657, -0.12394074962657776, 0.18384488756946996, 0.3084686476723894, 0.4316087583177539, 0.3289048080567337, -0.2501943741601341, 0.19810626956809926, 0.5281145973610051, 0.49922064822389306, 0.3847514113917196, 0.5154171615400677, 0.46439563693035196, -0.12517345118038087, 0.510146402633479, 0.6874537825704354, -0.21068521644829907, 0.460341486667849, 0.5001754733974403, 0.1491054670613725, 0.30804771658171315, 0.1130177364347829, 0.13341459586487156, 0.25378627463529413, 0.26129893277246413, 0.27174697317086033, 0.23068708563266213, 0.39465702612860065, 0.38756978636437633, 0.13451882693920547, -0.15346225267074934, 0.5140565716888433, 0.3723894933554234, 0.3507990908887737, 0.3698781728220092, -0.19333327181177723], "y": [-0.4093313246631645, 0.41930689880209443, 0.36304295801549913, -0.270530159248834, -0.2212524624770073, 0.16612978525889244, -0.3091324688347957, -0.05437961505898097, -0.39395075691105763, -0.1649026180149685, -0.08317296381406515, 0.2174848643756347, 0.20913144730953304, 0.3686980661597958, -0.2505901653672872, 0.5091762899704018, -0.35326805289012275, -0.24913654794180798, -0.3254652753007476, -0.11496516549676829, 0.3834074188988988, 0.38339182198505395, -0.24580009391868585, 0.06319493740747761, 0.2765049491033695, -0.28331448351254407, 0.057608296278100365, 0.09683499554607164, 0.35924694809435187, 0.1377405089141085, -0.3269814856093481, -0.0030068326444604534, -0.18618670952200975, 0.23555572958844292, -0.37083637775636974, -0.4149343979251529, -0.0356511393182167, 0.2865115143399301, 0.21527263506208424, -0.2702470291661137, -0.4181915131975785, 0.1782033666204493, 0.3302134896965148, 0.3173006951260257, 0.316410169104468, 0.20348606872791478, -0.07108227778688656, 0.013518759329265996, -0.03818930248944387, 0.2475556956930601, -0.001179472318164244, 0.22451637045658548, -0.2467469123654759, -0.019066713444229576, -0.13334234106220422, -0.43560414308730605, 0.01572300451156863, 0.10140899556848014, -0.0068293329280778034, -0.5490952479773991, -0.5125136026683522, -0.04982458006296985, 0.20163711476730786, -0.07224634740995502, -0.39784831379246527, 0.2007026567553393, 0.17720080356915846, 0.4817170743999518, -0.018718235635366818, 0.48118096813885874, -0.28365179716749117, -0.48958841870950315, 0.28374268729098967, -0.23981234887936068, -0.393291604649139, -0.5059183841423359, 0.1279565857914976, 0.645632767352481, -0.06632820168015102, -0.45786313695106207, 0.19759991312756783, -0.08626620363177942, -0.3749837364698802, -0.3671734866824597, -0.12374890837540756, 0.5451521679581282, -0.057489482272310745, 0.253434532764544, 0.34468739998965736, -0.08061889760707466, 0.41560491441282427, 0.04184942218715771, 0.2610169160848485, 0.03033767879239183, -0.04545492157858935, -0.1888753805181388, -0.02501993758689717, -0.26898612884354994, 0.14855743437491742, -0.3612939093484597, 0.22608092778532204, 0.5825401201434423, -0.0343566743869685, -0.3068385302719401, 0.31471752017508403, 0.3600680693155941, -0.5436984817687892, 0.25592239152728025, 0.5903568060493442, 0.545812880486154, -0.5154987067573058, -0.04936546929718627, 0.30328221696082264, -0.5024562026764692, -0.10730651911590966, 0.4456515178616222, -0.35044848784283883, 0.38639519783308474, 0.23308347521867942, 0.07075834843425116, -0.3308030010215437, -0.3319395297936585, 0.35668461718672867, 0.1985540121755788, -0.013799192292497316, 0.4347660463360561, 0.18032096235486283, -0.44506965747464877, 0.33343015728804637, -0.16909581218522363, 0.5681010224378731, 0.32855112552763843, -0.012474808126731008, 0.24978297100856237, -0.5578610559921731, -0.4542676242584957, -0.54800996004097, -0.28314658959049566, -0.24009859726783375, 0.3224397657778284, -0.3271217025842741, -0.18098372233639315, 0.2125333645847264, 0.4945334616343715, -0.4389468993206416, -0.5981140067136316, -0.031803735688725056, 0.27106065461957046, 0.20574551761055188, 0.17738990513085762, -0.016663451479262372, 0.3933223082110038, 0.45080527844208446, -0.19842839345569016, 0.3959281868347208, -0.33341233986446844, -0.12470456468248346, 0.009394551674599589, 0.4502890321157402, -0.527026623306298, 0.18802088220289834, -0.24920975486157637, -0.2660659988268734, -0.015522871575506313, -0.1798715648580141, 0.10361249643330751, 0.5563459093041456, -0.2956259595920717, -0.300620561100878, 0.6122451871696726, 0.12733152216812363, 0.15231924209886133, 0.05560732321426549, 0.3262095156481245, 0.13025743872431517, 0.5228895883249076, 0.20030235596515045, -0.10361460402842647, -0.3191806490839272, -0.25236993183960893, 0.5076082318443311, -0.44846600014102017, -0.47506692773144465, -0.21972096903309324, -0.37661200488830776, -0.17071588278942773, 0.1236986834679968, -0.49190047038356427, -0.5367204600410433, -0.026049289112533913, -0.42204066580412036, -0.4398173337257939, 0.3093650520131649, 0.34955497699411436, 0.1950049026813072, 0.40221313189781305, -0.22289527955724772, 0.4899161462529331, 0.4609242114001271, 0.3992613133634084, -0.35103369135878076, -0.3816674846549726, 0.41950239111490367, 0.1913301122494119, -0.4772313218360393, -0.3597625827003353, 0.07975393786879806, -0.2338875099477375, 0.07588657242648753], "z": [-0.24601029477330255, 0.1727843164088678, 0.23032227053208384, -0.08933660561636969, 0.13570746359138156, 0.6231341071338781, -0.1034299518402763, 0.21324363788814962, -0.4405190320979334, 0.24215810001598786, 0.2086856142391251, 0.6328245683498893, 0.599922971306284, 0.37761945199961616, 0.1525208658940514, 0.09629710811958649, -0.3680721007714444, 0.1498054882943201, 0.015562468934048482, 0.4155940103828949, 0.6215060985904455, 0.16483281200469244, 0.21224535819254572, 0.43408937806431536, 0.42718327083657165, -0.21015977335853375, 0.3204255811514066, 0.5373724419687406, 0.2488399162675095, 0.47913870698280103, -0.014353013982801945, 0.5624844859231741, 0.14620611276275317, 0.21530319388933106, -0.3297755536893024, -0.4097457527810226, 0.34130655001336807, 0.30350166737365325, 0.6674244514866434, -0.09968790490557074, -0.29543227975698905, 0.6020814456686238, 0.2598266835587006, -0.10309955032218786, 0.4894951573047241, 0.5955953481276542, 0.45616143751812543, 0.5434462926853108, 0.4082728330763833, 0.6170885034692835, 0.15507347661013526, 0.33271158924191563, -0.07700885324001507, 0.47848718592807216, 0.32846256757351605, -0.3687044259654353, 0.654819061603093, 0.6376416032848667, 0.5657201021760389, 0.15197561811856744, 0.1514953310064429, -0.3014492797769437, -0.20958961813445742, -0.1078859606280915, 0.16330722216393231, -0.1557664274483549, -0.2233490471693941, -0.014179757586183363, -0.32752735120146415, -0.25772917132528256, 0.10573524004075029, 0.17213962828778878, 0.3878467370213848, -0.024479180422175048, 0.14998824697053992, 0.17928795121046578, -0.3109300443740245, -0.4743119275356093, 0.11461277664175731, 0.12823456451795742, -0.18929320134502528, 0.04526361367989733, 0.013123108888235308, 0.15616769723746235, -0.21468545129886052, -0.2913178926066388, -0.17034425091025235, 0.43813123257018727, 0.14238298383801393, 0.059044412227161724, -0.31799282587338285, -0.17883588778339726, -0.27807608242917764, -0.12398840284047424, 0.06565668835704559, -0.017739149400699514, -0.164717982640179, -0.0072419892005033265, 0.15449377686296456, 0.09220812910503139, -0.24642502722018036, -0.5117966539028583, -0.3747752915295728, 0.06503958074347715, 0.21955813613052852, -0.39416654292352976, 0.17534205032456202, -0.38963328180459555, -0.5382709345128619, -0.4908387649981039, 0.1134841098099385, -0.2395685848852392, -0.102020211932738, 0.16815640756171538, -0.01176547325666831, -0.4413066870164942, 0.14489519596578465, 0.26043870890981613, 0.14237615818807917, 0.01055096307853505, 0.010459363004808347, 0.13908841653794884, -0.36423043656984816, 0.014913835288383723, -0.3439320450416459, -0.11778546102975021, -0.24612218553671497, 0.10544800966285513, -0.19197545136814245, -0.1420436677251921, -0.4140180771948491, -0.12367326859096316, -0.17684938808210002, -0.23686361154547986, 0.14474594248179817, 0.16467031361626536, 0.17637040815925142, -0.030216762974214353, -0.016841956132575837, -0.3742840679288535, 0.12413520256524799, -0.02645496620610217, -0.2070222358959625, -0.4180657231631205, 0.16342172535043584, 0.15639518843147857, -0.2948892225128257, -0.256384887292292, 0.04625359259636849, -0.18301702313040555, -0.3364649144456949, -0.10890859836141033, -0.37926956254394484, -0.33973285746738896, -0.4024781369664552, 0.008385092531555132, -0.2964917757244602, -0.27395903056528725, -0.4397190523787831, 0.1746820509266036, -0.2537027396198354, 0.09009965843084931, 0.08744651075543168, -0.1942965261361198, -0.07943039251461863, -0.3579395189355709, -0.377337648805607, 0.035961844892180156, -0.3739548818759193, -0.4102955463199313, -0.22961538461722633, 0.16780933495082284, -0.3223869122812845, 0.3685350900011797, -0.29353766367254847, -0.4924437046621683, -0.22207431109293324, -0.037526263379104696, 0.12620257653560504, -0.2606009328864567, -0.47059961107843035, 0.10049447686023838, 0.16194542751755558, -0.030697329869629343, 0.04012885013223387, -0.0908172454563232, -0.369571014159478, 0.1732396377657625, 0.12233881470690489, -0.2519178359284662, 0.16601608074699203, 0.14870190846998418, -0.32738625779988967, -0.34882547943378583, -0.23419516719609604, -0.4151688663392297, 0.05386555732219703, -0.45211764269885096, -0.4027858933005524, -0.33736801009819073, 0.09201432838273556, 0.1387812592381812, -0.39864779932915956, 0.1658376815045072, 0.14946768108455477, 0.1444226020011473, -0.2330706861527985, -0.010036009045528335, 0.36399700224226644], "mode": "markers", "marker": {"size": 5, "line": {"width": 1}, "color": 1, "colorscale": "Jet", "opacity": 0.8}, "name": "Abnormal", "text": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208], "hoverinfo": "text+name"}], {"title": "3D Representation of the patients characteristics using Spectral Embedding", "hovermode": "closest", "showlegend": true}, {"showLink": true, "linkText": "Export to plot.ly"})});</script>


Now that's a funny shape...  
This is the clearest classes separation we've seen here. The "nose" that comes out seems very class pure with only 3 points classified as normal in a cluster of **only** abnormal points. In this case, studying these points included in the abnormal cluster could also provide insight on this _special_ cases. But there is much more that we can do with this !  
See the triangular shape of the "base" ? We could study how the different characteristic of a patient change depending on which tip of the triangle is the closes to him, giving yet another visual representation of possible sub-classes in the back pains.  
We still see that the normal points anre mixed in with the abnormal points, that reinforces my hypothesis that some abnormal points were classified as such just becasue of one single parameter that was too high, something to look into...

## Conclusion

I hope you enjoyed reading this notebook as much as I enjoyed writing it, feel free to get inspired from it and use the code in your own notebooks, I'd love to see what you make of it.  
If you learnt something or just had a good reading experience, don't forget to upvote and check out my other kernels !
